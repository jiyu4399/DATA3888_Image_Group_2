{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d924996-e8ce-463b-af5b-f4ad957fd61e",
   "metadata": {},
   "source": [
    "# Part A\n",
    "You should only need to run this once, after that can simply load in the saved images as done in part B."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "86e6320a0b9f9623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "data_dir = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"data_processed\"))\n",
    "file_path = os.path.join(data_dir, \"morphology_focus.tif\")"
   ],
   "id": "8e30b0dd29b53a1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "# Open the .tif file\n",
    "img = Image.open(os.path.join(data_dir, \"morphology_focus.tif\"))\n",
    "\n",
    "\n",
    "# Convert to numpy array\n",
    "image_array = np.array(img)\n",
    "\n",
    "# Calculate the quantile to scale intensity\n",
    "quantile_value = np.quantile(image_array, 0.99)\n",
    "\n",
    "# Scale intensity by dividing by the quantile\n",
    "scaled_image_array = image_array / quantile_value\n",
    "\n",
    "# Clip values greater than 1 to ensure image is within [0, 1] range\n",
    "img2 = np.clip(scaled_image_array, 0, 1)\n",
    "\n",
    "# Display the scaled image\n",
    "# plt.imshow(img2, cmap='gray')  # Uncomment this line and below to display the whole image (this runs slow)\n",
    "# plt.show()"
   ],
   "id": "87630839d29546bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cell_boundaries = pd.read_csv(os.path.join(data_dir, \"cell_boundaries.csv.gz\"))\n",
    "cell_boundaries[\"vertex_x_trans\"] = cell_boundaries[\"vertex_x\"].apply(\n",
    "    lambda x: int(x / 0.2125)\n",
    ")\n",
    "cell_boundaries[\"vertex_y_trans\"] = cell_boundaries[\"vertex_y\"].apply(\n",
    "    lambda x: int(x / 0.2125)\n",
    ")"
   ],
   "id": "5e51edf1dbe0a3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clusters = pd.read_csv(os.path.join(data_dir, \"clusters.csv\"))\n",
    "ncells = clusters.shape[0]\n",
    "ncells"
   ],
   "id": "90d70a42cb3b64f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "img_pix = np.array(img)",
   "id": "4e3fc7583bb83986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(2024)\n",
    "\n",
    "ncells_subset = 1000\n",
    "\n",
    "cells_subset = random.sample(range(ncells), ncells_subset)\n",
    "\n",
    "# clusters.loc[cells_subset, \"Cluster\"]\n",
    "\n",
    "for i in cells_subset:\n",
    "    # extract the boundary vertices for the selected cell\n",
    "    bounds_i = cell_boundaries.loc[cell_boundaries[\"cell_id\"] == i]\n",
    "\n",
    "    # extract the cluster value for the selected cell\n",
    "    clustval_i = clusters.loc[i, \"Cluster\"]\n",
    "\n",
    "    # extract the pixel intensities for the area covering the cell boundary\n",
    "    img_sub = img_pix[\n",
    "        min(bounds_i[\"vertex_y_trans\"]) : max(bounds_i[\"vertex_y_trans\"]),\n",
    "        min(bounds_i[\"vertex_x_trans\"]) : max(bounds_i[\"vertex_x_trans\"]),\n",
    "    ]\n",
    "\n",
    "    # normalise the pixel intensities according to 99th percentile\n",
    "    img_sub_norm = img_sub / np.quantile(img_sub, 0.99)\n",
    "\n",
    "    # as an example, display the image for the first selected cell\n",
    "    if i in cells_subset[0:5]:\n",
    "        print(f\"Displaying image for cell {i}\")\n",
    "        # plt.imshow(img_sub_norm, cmap=\"gray\")  # Uncomment this line and below to display sample cell images\n",
    "        # plt.show()\n",
    "\n",
    "    # create directory for images if it doesn't exist\n",
    "    cell_dir = os.path.join(os.getcwd(), data_dir, \"cell_images_py\")\n",
    "    if not os.path.exists(cell_dir):\n",
    "        os.mkdir(cell_dir)\n",
    "\n",
    "    # create directory for cluster if it doesn't exist\n",
    "    clust_dir = os.path.join(cell_dir, f\"cluster_{clustval_i}\")\n",
    "    if not os.path.exists(clust_dir):\n",
    "        os.mkdir(clust_dir)\n",
    "\n",
    "    # save extracted image as a png file\n",
    "    plt.imsave(os.path.join(clust_dir, f\"cell_{i}.png\"), img_sub_norm, cmap=\"gray\")"
   ],
   "id": "1a089df2c5bcbed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "img_sub_norm",
   "id": "38636a1c1757185c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from skimage import filters, measure, morphology, segmentation, color\n",
    "\n",
    "\n",
    "# Apply a Gaussian filter for smoothing (reduce noise)\n",
    "img_sub_smooth = filters.gaussian(img_sub_norm, sigma=1)\n",
    "\n",
    "# Edge detection using Sobel filter\n",
    "edges = filters.sobel(img_sub_smooth)\n",
    "\n",
    "# Segmentation using Otsu thresholding\n",
    "threshold_value = filters.threshold_otsu(img_sub_smooth)\n",
    "segmented = img_sub_smooth > threshold_value\n",
    "\n",
    "# Morphological operations for cleaning up the segmentation\n",
    "cleaned = morphology.remove_small_objects(segmented, min_size=50)\n",
    "cleaned = morphology.closing(cleaned, morphology.disk(4))\n",
    "\n",
    "# Label the objects (cells)\n",
    "labels = measure.label(cleaned)\n",
    "\n",
    "# Color the labels to visually distinguish different objects\n",
    "colored_labels = color.label2rgb(labels, image=img_sub_smooth, bg_label=0)\n",
    "\n",
    "\n",
    "# Display the original and processed images side-by-side\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "ax[0].imshow(img_sub_norm, cmap=\"gray\")\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "ax[1].imshow(img_sub_smooth, cmap=\"gray\")\n",
    "ax[1].set_title(\"Smoothed Image\")\n",
    "\n",
    "ax[2].imshow(edges, cmap=\"gray\")\n",
    "ax[2].set_title(\"Edges\")\n",
    "\n",
    "ax[3].imshow(colored_labels)\n",
    "ax[3].set_title(\"Segmented and Labeled\")\n",
    "\n",
    "for a in ax:\n",
    "    a.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "76348b8d35f2ec58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4104870806b8a272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part B",
   "id": "6eca9c79db238d9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "2255616979a1625b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ],
   "id": "3a1787b3d6b9fa95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATA_DIRECTORY = data_dir\n",
    "CELL_IMAGES = os.path.join(DATA_DIRECTORY, \"cell_images_py\")\n",
    "SIZE = (50, 50)"
   ],
   "id": "85551c48c6f43a32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading in and displaying images",
   "id": "4bf4d4348887c34d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_cluster_images(cluster: int, size=(None, None)):\n",
    "    DIR = os.path.join(CELL_IMAGES, f\"cluster_{cluster}\")\n",
    "    files = [os.path.join(DIR, file) for file in os.listdir(DIR)]\n",
    "    return [\n",
    "        Image.open(file).convert(\"L\")\n",
    "        if None in size\n",
    "        else Image.open(file).convert(\"L\").resize(size)\n",
    "        for file in files\n",
    "    ]"
   ],
   "id": "48a582975ac86462"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# cluster_A_files = os.path.join(CELL_IMAGES, os.listdir(os.path.join(CELL_IMAGES, 'cluster_8')))\n",
    "cluster_A_files = [\n",
    "    os.path.join(os.path.join(CELL_IMAGES, \"cluster_8\"), file)\n",
    "    for file in os.listdir(os.path.join(CELL_IMAGES, \"cluster_8\"))\n",
    "]\n",
    "cluster_B_files = [\n",
    "    os.path.join(os.path.join(CELL_IMAGES, \"cluster_13\"), file)\n",
    "    for file in os.listdir(os.path.join(CELL_IMAGES, \"cluster_13\"))\n",
    "]"
   ],
   "id": "d6153fd260a7cd2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cluster_A_images = get_cluster_images(8)\n",
    "cluster_B_images = get_cluster_images(13)\n",
    "cluster_A_images_resized = get_cluster_images(8, size=SIZE)\n",
    "cluster_B_images_resized = get_cluster_images(13, size=SIZE)"
   ],
   "id": "3b2a1b800813ae60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display_images_in_grid(images, ncols=3, cmap=None):\n",
    "    nrows = (len(images) + ncols - 1) // ncols\n",
    "    fig_height = (\n",
    "        nrows * 1\n",
    "    )  # Adjust the multiplier as needed to control spacing between rows\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, fig_height))\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        if not cmap:\n",
    "            ax.imshow(image)\n",
    "        else:\n",
    "            ax.imshow(image, cmap=cmap)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.03)  # Adjust spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_images_in_grid(cluster_A_images_resized, ncols=10, cmap=\"gray\")"
   ],
   "id": "78d27da10c0bb7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cell boundary and masking",
   "id": "2d9657eb506f4724"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "cd70266c6ca9c680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from skimage.draw import polygon2mask"
   ],
   "id": "5b7aedec2d33971f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cell_boundaries_raw = pd.read_csv(\n",
    "    os.path.join(DATA_DIRECTORY, \"cell_boundaries.csv.gz\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_cluster_cell_ids(cluster: int):\n",
    "    DIR = os.path.join(CELL_IMAGES, f\"cluster_{cluster}\")\n",
    "    files = [os.path.join(DIR, file) for file in os.listdir(DIR)]\n",
    "    pattern = r\"cell_(\\d+)\\.png\"\n",
    "    return [int(re.search(pattern, file).group(1)) for file in files]\n",
    "\n",
    "\n",
    "cluster_A_cell_ids = get_cluster_cell_ids(8)\n",
    "cluster_B_cell_ids = get_cluster_cell_ids(13)"
   ],
   "id": "15420ed9bba9dd4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Masking one cell",
   "id": "70e689ebf22472cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cell_boundaries = cell_boundaries_raw.loc[\n",
    "    cell_boundaries_raw[\"cell_id\"].isin(cluster_A_cell_ids + cluster_B_cell_ids)\n",
    "]"
   ],
   "id": "d5db7ecfd9668b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_masked_image(\n",
    "    image: Image, cell_id: int, cell_boundaries: pd.core.frame.DataFrame\n",
    "):\n",
    "    height, width = np.array(image).shape\n",
    "    cell_boundary = cell_boundaries[cell_boundaries[\"cell_id\"] == cell_id].copy()\n",
    "    cell_boundary[\"vertex_x_scaled\"] = 1 + (\n",
    "        (cell_boundary[\"vertex_x\"] - cell_boundary[\"vertex_x\"].min()) * scaling_factor\n",
    "    )\n",
    "    cell_boundary[\"vertex_y_scaled\"] = 1 + (\n",
    "        (cell_boundary[\"vertex_y\"] - cell_boundary[\"vertex_y\"].min()) * scaling_factor\n",
    "    )\n",
    "    polygon = np.array(\n",
    "        list(zip(cell_boundary[\"vertex_y_scaled\"], cell_boundary[\"vertex_x_scaled\"]))\n",
    "    )\n",
    "    mask = polygon2mask((height, width), polygon)\n",
    "    masked_image = np.ma.masked_array(np.copy(img_A), np.invert(mask), fill_value=0)\n",
    "    return masked_image"
   ],
   "id": "6729ab75d3641152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "index = 0\n",
    "\n",
    "cell_boundary = cell_boundaries[\n",
    "    cell_boundaries[\"cell_id\"] == cluster_A_cell_ids[index]\n",
    "].copy()\n",
    "img_A = cluster_A_images[index].convert(\"L\")\n",
    "img_A_resized = cluster_A_images_resized[index].convert(\"L\")\n",
    "\n",
    "# Calculate scaling factor\n",
    "scaling_factor = 1 / 0.22\n",
    "height, width = np.array(img_A).shape\n",
    "\n",
    "# Calculate scaled x and y coordinates\n",
    "cell_boundary[\"vertex_x_scaled\"] = (\n",
    "    (\n",
    "        1\n",
    "        + (\n",
    "            (cell_boundary[\"vertex_x\"] - cell_boundary[\"vertex_x\"].min())\n",
    "            * scaling_factor\n",
    "        )\n",
    "    )\n",
    "    * 50\n",
    "    / width\n",
    ")\n",
    "cell_boundary[\"vertex_y_scaled\"] = (\n",
    "    (\n",
    "        1\n",
    "        + (\n",
    "            (cell_boundary[\"vertex_y\"] - cell_boundary[\"vertex_y\"].min())\n",
    "            * scaling_factor\n",
    "        )\n",
    "    )\n",
    "    * 50\n",
    "    / height\n",
    ")\n",
    "\n",
    "# Visualize the image\n",
    "plt.clf()\n",
    "plt.close()\n",
    "plt.imshow(img_A_resized, cmap=\"gray\")\n",
    "plt.plot(\n",
    "    cell_boundary[\"vertex_x_scaled\"],\n",
    "    cell_boundary[\"vertex_y_scaled\"],\n",
    "    \"y-o\",\n",
    "    linewidth=3,\n",
    ")\n",
    "plt.show()"
   ],
   "id": "51201eacc63606a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "height, width = np.array(img_A.convert(\"L\")).shape\n",
    "polygon = np.array(\n",
    "    list(zip(cell_boundary[\"vertex_y_scaled\"], cell_boundary[\"vertex_x_scaled\"]))\n",
    ")\n",
    "mask = polygon2mask((height, width), polygon)\n",
    "masked_image = np.ma.masked_array(np.copy(img_A), np.invert(mask), fill_value=0)\n",
    "\n",
    "# Display the masked image\n",
    "# result = np.where(masked_image.mask, 0, masked_image)  # image with mask\n",
    "plt.close()\n",
    "result = np.where(masked_image.mask, 0, 1)  # just the mask\n",
    "plt.imshow(result, cmap=\"gray\")\n",
    "plt.show()"
   ],
   "id": "84774f7136d9ba6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Masking all cells",
   "id": "f88198be5086cea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_cluster_masked_images(cluster: int, size=(None, None)):\n",
    "    images = get_cluster_images(cluster)\n",
    "    if None not in size:  # get resized images if size is specified\n",
    "        images_resized = get_cluster_images(cluster, size)\n",
    "\n",
    "    cell_ids = get_cluster_cell_ids(cluster)\n",
    "    cell_boundaries = cell_boundaries_raw.loc[\n",
    "        cell_boundaries_raw[\"cell_id\"].isin(cell_ids)\n",
    "    ].copy()\n",
    "\n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = 1 / 0.22\n",
    "\n",
    "    result = {}\n",
    "    for cell_id, image, image_resized in zip(cell_ids, images, images_resized):\n",
    "        # get shape of original image (without resize)\n",
    "        height, width = np.array(image).shape\n",
    "        # scaling factor for cell boundary, only if size is given\n",
    "        resize_x = 50 / width if None not in size else 1\n",
    "        resize_y = 50 / height if None not in size else 1\n",
    "\n",
    "        # get cell boundary coordinates for the image\n",
    "        cell_boundary = cell_boundaries.loc[\n",
    "            cell_boundaries[\"cell_id\"] == cell_id\n",
    "        ].copy()\n",
    "        # Calculate scaled x and y coordinates\n",
    "        cell_boundary[\"vertex_x_scaled\"] = (\n",
    "            1\n",
    "            + (\n",
    "                (cell_boundary[\"vertex_x\"] - cell_boundary[\"vertex_x\"].min())\n",
    "                * scaling_factor\n",
    "            )\n",
    "        ) * resize_x\n",
    "        cell_boundary[\"vertex_y_scaled\"] = (\n",
    "            1\n",
    "            + (\n",
    "                (cell_boundary[\"vertex_y\"] - cell_boundary[\"vertex_y\"].min())\n",
    "                * scaling_factor\n",
    "            )\n",
    "        ) * resize_y\n",
    "\n",
    "        # create the mask\n",
    "        polygon = np.array(\n",
    "            list(\n",
    "                zip(cell_boundary[\"vertex_y_scaled\"], cell_boundary[\"vertex_x_scaled\"])\n",
    "            )\n",
    "        )\n",
    "        mask = polygon2mask((height, width) if None in size else SIZE, polygon)\n",
    "        # mask the image\n",
    "        masked_image = np.ma.masked_array(\n",
    "            np.copy(image if None in size else image_resized),\n",
    "            np.invert(mask),\n",
    "            fill_value=0,\n",
    "        )\n",
    "        result[cell_id] = np.where(masked_image.mask, 0, masked_image)\n",
    "\n",
    "    return result"
   ],
   "id": "b5a51eabfb5c30b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "masked_cells = {\n",
    "    cluster: get_cluster_masked_images(cluster, size=SIZE) for cluster in range(1, 29)\n",
    "}"
   ],
   "id": "47140e82b6b4c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "key_to_access = 10210\n",
    "image_data = masked_cells[1][key_to_access]\n",
    "\n",
    "# Assuming the retrieved image_data is a NumPy array, you can check its shape\n",
    "print(image_data.shape)\n",
    "\n",
    "\n",
    "# display_images_in_grid(masked_cells[1])\n",
    "plt.close()\n",
    "plt.imshow(masked_cells[1][12200], cmap=\"gray\")"
   ],
   "id": "5a0fed502e932edc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data transformation 1: use scikit image",
   "id": "870773747cfed2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, measure, morphology, segmentation, color\n",
    "\n",
    "\n",
    "def process_and_display_segmented_image(\n",
    "    img, sigma=1, min_obj_size=50, closing_disk_size=4\n",
    "):\n",
    "    # Apply Gaussian filter for smoothing (reduce noise)\n",
    "    img_sub_smooth = filters.gaussian(img, sigma=sigma)\n",
    "\n",
    "    # Edge detection using Sobel filter\n",
    "    edges = filters.sobel(img)\n",
    "\n",
    "    # Segmentation using Otsu thresholding\n",
    "    threshold_value = filters.threshold_otsu(img)\n",
    "    segmented = img_sub_smooth > threshold_value\n",
    "\n",
    "    # Morphological operations for cleaning up the segmentation\n",
    "    cleaned = morphology.remove_small_objects(segmented, min_size=min_obj_size)\n",
    "    cleaned = morphology.closing(cleaned, morphology.disk(closing_disk_size))\n",
    "\n",
    "    # Label the objects (cells)\n",
    "    labels = measure.label(cleaned)\n",
    "\n",
    "    # Color the labels to visually distinguish different objects\n",
    "    colored_labels = color.label2rgb(labels, image=img_sub_smooth, bg_label=0)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    ax[0].imshow(img, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "\n",
    "    ax[1].imshow(img_sub_smooth, cmap=\"gray\")\n",
    "    ax[1].set_title(\"Smoothed Image\")\n",
    "\n",
    "    ax[2].imshow(edges, cmap=\"gray\")\n",
    "    ax[2].set_title(\"Edges\")\n",
    "\n",
    "    ax[3].imshow(colored_labels)\n",
    "    ax[3].set_title(\"Segmented and Labeled\")\n",
    "\n",
    "    for a in ax:\n",
    "        a.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "img = masked_cells[1][key_to_access]\n",
    "process_and_display_segmented_image(img)"
   ],
   "id": "f3e3a592a4228691"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data transformation 2: image transformation canny contour using opencv",
   "id": "5e254eb1604166c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def process_and_display_contours(\n",
    "    img,\n",
    "    canny_threshold1=10,\n",
    "    canny_threshold2=200,\n",
    "    contour_color=(0, 255, 0),\n",
    "    contour_thickness=1,\n",
    "):\n",
    "    # Use Canny edge detection\n",
    "    canny = cv2.Canny(img, canny_threshold1, canny_threshold2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(canny, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    print(\"Number of Contours =\", len(contours))\n",
    "\n",
    "    # Create an image copy to draw contours on\n",
    "    display_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.drawContours(display_img, contours, -1, contour_color, contour_thickness)\n",
    "\n",
    "    # Convert BGR to RGB for displaying in matplotlib\n",
    "    display_img_rgb = cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.close()\n",
    "    plt.imshow(display_img_rgb)\n",
    "    plt.title(\"Contours\")\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    return display_img\n",
    "\n",
    "\n",
    "img = masked_cells[8][33839]\n",
    "process_and_display_contours(img)"
   ],
   "id": "602b2e3857bdbe11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data transformation 3:  image transformation GaussianBlur contour using opencv",
   "id": "75f9a5a78361a174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def process_and_display_contours(\n",
    "    img,\n",
    "    blur_size=(25, 25),\n",
    "    threshold_value=25,\n",
    "    contour_color=(0, 0, 255),\n",
    "    contour_thickness=2,\n",
    "):\n",
    "    # Apply Gaussian blur to help with contour detection\n",
    "    blur = cv2.GaussianBlur(img, blur_size, 0)\n",
    "\n",
    "    # Apply a binary threshold to the blurred image\n",
    "    _, binary = cv2.threshold(blur, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours from the binary image\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Find the largest contour by area\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        Contours_gray = cv2.drawContours(\n",
    "            img, largest_contour, -1, contour_color, contour_thickness\n",
    "        )\n",
    "\n",
    "        # Convert the image to RGB for displaying with matplotlib\n",
    "        img_rgb = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "        # Create an image copy to draw the largest contour\n",
    "        contour_img = cv2.drawContours(\n",
    "            img_rgb, [largest_contour], -1, contour_color, contour_thickness\n",
    "        )\n",
    "\n",
    "        # Display the result using matplotlib\n",
    "        plt.close()\n",
    "        plt.imshow(contour_img)\n",
    "        plt.axis(\"off\")  # Hide axes\n",
    "        plt.show()\n",
    "        return result\n",
    "    else:\n",
    "        print(\"No contours found.\")\n",
    "\n",
    "\n",
    "img = masked_cells[8][33839]\n",
    "process_and_display_contours(img)"
   ],
   "id": "9c2939b32b31b8c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming masked_cells[1][10366] contains the image data you want to work with.\n",
    "img = masked_cells[1][10366]\n",
    "\n",
    "# Check if the image is color and convert to grayscale\n",
    "if len(img.shape) == 3:\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "else:\n",
    "    img_gray = img  # If already grayscale, use it directly\n",
    "\n",
    "# Apply Gaussian blur to help with contour detection\n",
    "blur = cv2.GaussianBlur(img_gray, (25, 25), 0)\n",
    "\n",
    "# Apply a binary threshold to the blurred image\n",
    "ret, binary = cv2.threshold(blur, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Find contours from the binary image\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    ")\n",
    "\n",
    "# Find the index of the largest object based on the length of the contour\n",
    "if contours:\n",
    "    obj_index = contours.index(max(contours, key=len))\n",
    "\n",
    "    # Draw the largest contour on the original image in red\n",
    "    contour_img = cv2.drawContours(\n",
    "        img.copy(), contours, obj_index, (0, 0, 255), 2\n",
    "    )  # Red contour\n",
    "\n",
    "    # Convert the image from BGR to RGB for displaying using matplotlib\n",
    "    contour_img_rgb = cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the result using matplotlib\n",
    "    plt.close()\n",
    "    plt.imshow(contour_img_rgb)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No contours found.\")"
   ],
   "id": "c339ebc750806dfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deep learning",
   "id": "f833bb6a30a31d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "id": "b89984331d915e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Make the dataset",
   "id": "ddcc70eea3e13cb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_dict):\n",
    "        self.images_dict = images_dict\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for cluster, images in images_dict.items():\n",
    "            self.images.extend(images.values())\n",
    "            self.labels.extend([cluster - 1] * len(images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert image to PyTorch tensor and normalize if necessary\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float32) / 255.0\n",
    "        image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "        # Convert label to PyTorch tensor\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return image_tensor, label_tensor"
   ],
   "id": "13b30cedca8b1eb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load data and create dataloaders",
   "id": "8e3545203cc49dc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "custom_dataset = CustomDataset(masked_cells)\n",
    "\n",
    "# Define the sizes of the training, validation, and testing sets\n",
    "train_size = int(0.6 * len(custom_dataset))  # 60% for training\n",
    "val_size = int(0.2 * len(custom_dataset))  # 20% for validation\n",
    "test_size = len(custom_dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    custom_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create DataLoader for testing set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "9126241a01abd7e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Make the models",
   "id": "d5a3471e77836607"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Random simple model",
   "id": "45eaea91ddf187b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 6 * 6, 128\n",
    "        )  # Adjust input size based on your image size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 6 * 6)  # Adjust input size based on your image size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "6aa92748e51a99eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lab 4b Model",
   "id": "ff4560b06d6c0438"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LabModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 12 * 12, 128\n",
    "        )  # Adjust input size based on your image size\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, 28)  # Assuming 2 classes for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 64 * 12 * 12)  # Adjust input size based on your image size\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "baed348ec8351243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### AlexNet",
   "id": "f931274ce1d4002b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(32 * 12 * 12, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(-1, 32 * 12 * 12)\n",
    "        return self.classifier(x)\n",
    "        # return x"
   ],
   "id": "36595ddb4c12a601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "3da10938dd417da0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store training and validation loss and accuracy\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        epoch_train_accuracy = correct_train / total_train\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_accuracy = correct_val / total_val\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "        # Print progress\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {epoch_train_accuracy:.4f}, \"\n",
    "            f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Return training and validation losses and accuracies\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ],
   "id": "c1d8770a428d7b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.size())\n",
    "    break"
   ],
   "id": "f4e969acab718de0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = LabModel()\n",
    "t_loss, v_loss, t_acc, v_acc = train_model(model, train_loader, val_loader)\n",
    "lab_model_results = {\"t_loss\": t_loss, \"v_loss\": v_loss, \"t_acc\": t_acc, \"v_acc\": v_acc}"
   ],
   "id": "16bb0a44eb42ef2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_plots(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    with plt.style.context(\"dark_background\"):\n",
    "        # Generate x-axis values (epochs)\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        ax1.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "        ax1.plot(epochs, val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "\n",
    "        # Add jitter to the data points for better visualization\n",
    "        ax1.scatter(epochs, train_losses, color=\"blue\")\n",
    "        ax1.scatter(epochs, val_losses, color=\"orange\")\n",
    "\n",
    "        # Add labels and legend for the first subplot\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Training and Validation Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot training and validation accuracies\n",
    "        ax2.plot(epochs, train_accuracies, label=\"Training Accuracy\", marker=\"o\")\n",
    "        ax2.plot(epochs, val_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n",
    "\n",
    "        # Add jitter to the data points for better visualization\n",
    "        ax2.scatter(epochs, train_accuracies, color=\"green\")\n",
    "        ax2.scatter(epochs, val_accuracies, color=\"red\")\n",
    "\n",
    "        # Add labels and legend for the second subplot\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Training and Validation Accuracy\")\n",
    "        ax2.legend()\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()"
   ],
   "id": "fc775c83ad84d5fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e851cb-eae4-4135-9820-95a83387c5b0",
   "metadata": {},
   "outputs": [],
   "source": "generate_plots(t_loss, v_loss, t_acc, v_acc)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
