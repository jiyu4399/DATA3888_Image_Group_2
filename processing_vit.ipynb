{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d924996-e8ce-463b-af5b-f4ad957fd61e",
   "metadata": {},
   "source": [
    "# Part A\n",
    "You should only need to run this once, after that can simply load in the saved images as done in part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14404a2-0cc1-4354-b511-edae3599c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# import javabridge\n",
    "# import bioformats\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "data_dir = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"data_processed\"))\n",
    "dir_path = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"data_raw_big/Xenium_V1_FF_Mouse_Brain_MultiSection_1_outs\"))\n",
    "file_path = os.path.join(data_dir, \"morphology_focus.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26371f-2418-4c92-9475-3bbefc42a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Open the .tif file\n",
    "img = Image.open(os.path.join(data_dir, \"morphology_focus.tif\"))\n",
    "# img = Image.open(os.path.join(dir_path, \"morphology_focus.tif\"))\n",
    "\n",
    "\n",
    "# Specify file paths\n",
    "tif_file_path = \"../data_processed/morphology_focus.tif\"\n",
    "ome_tif_file_path = \"../data_raw/morphology_focus.ome.tif\"\n",
    "\n",
    "# # Check if the TIFF file already exists\n",
    "# if not os.path.exists(tif_file_path):\n",
    "#     # Start the JVM with additional memory\n",
    "#     javabridge.start_vm(class_path=bioformats.JARS, max_heap_size=\"10G\")\n",
    "\n",
    "#     try:\n",
    "#         # Read the OME-TIFF image\n",
    "#         with bioformats.ImageReader(ome_tif_file_path) as reader:\n",
    "#             img = reader.read()\n",
    "        \n",
    "#         # Normalize the image if necessary\n",
    "#         img = img / 255.0  # Normalization, if needed\n",
    "\n",
    "#         # Convert to PIL image and save as TIFF\n",
    "#         pil_img = Image.fromarray((img * 255).astype(\"uint8\"))  # Convert to 8-bit unsigned int\n",
    "#         pil_img.save(tif_file_path, format=\"TIFF\")\n",
    "    \n",
    "#     finally:\n",
    "#         # Stop the JVM\n",
    "#         javabridge.kill_vm()\n",
    "\n",
    "\n",
    "# Convert to numpy array\n",
    "image_array = np.array(img)\n",
    "\n",
    "# Calculate the quantile to scale intensity\n",
    "quantile_value = np.quantile(image_array, 0.99)\n",
    "\n",
    "# Scale intensity by dividing by the quantile\n",
    "scaled_image_array = image_array / quantile_value\n",
    "\n",
    "# Clip values greater than 1 to ensure image is within [0, 1] range\n",
    "img2 = np.clip(scaled_image_array, 0, 1)\n",
    "\n",
    "# Display the scaled image\n",
    "# plt.imshow(img2, cmap='gray')  # Uncomment this line and below to display the whole image (this runs slow)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073bb61-aa55-4703-bc67-946a0fe0d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries = pd.read_csv(os.path.join(data_dir, \"cell_boundaries.csv.gz\"))\n",
    "cell_boundaries[\"vertex_x_trans\"] = cell_boundaries[\"vertex_x\"].apply(\n",
    "    lambda x: int(x / 0.2125)\n",
    ")\n",
    "cell_boundaries[\"vertex_y_trans\"] = cell_boundaries[\"vertex_y\"].apply(\n",
    "    lambda x: int(x / 0.2125)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80df594-65c0-42d2-ae23-adbd7b2c527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv(os.path.join(data_dir, \"clusters.csv\"))\n",
    "ncells = clusters.shape[0]\n",
    "ncells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f1d77-d687-4497-aed5-198d9415af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pix = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67cf77-541f-4439-9759-7a8e93284115",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2024)\n",
    "\n",
    "ncells_subset = 1000\n",
    "\n",
    "cells_subset = random.sample(range(ncells), ncells_subset)\n",
    "\n",
    "clusters.loc[cells_subset, \"Cluster\"]\n",
    "\n",
    "for i in range(1, ncells):  # change range(1, ncells) to cells_subset (or the other way around) if you want a sample of 1000 images\n",
    "    \n",
    "    # extract the boundary vertices for the selected cell\n",
    "    bounds_i = cell_boundaries.loc[cell_boundaries[\"cell_id\"] == i]\n",
    "\n",
    "    # extract the cluster value for the selected cell\n",
    "    clustval_i = clusters.loc[i, \"Cluster\"]\n",
    "\n",
    "    # extract the pixel intensities for the area covering the cell boundary\n",
    "    img_sub = img_pix[\n",
    "        min(bounds_i[\"vertex_y_trans\"]) : max(bounds_i[\"vertex_y_trans\"]),\n",
    "        min(bounds_i[\"vertex_x_trans\"]) : max(bounds_i[\"vertex_x_trans\"]),\n",
    "    ]\n",
    "\n",
    "    # normalise the pixel intensities according to 99th percentile\n",
    "    img_sub_norm = img_sub / np.quantile(img_sub, 0.99)\n",
    "\n",
    "    # as an example, display the image for the first selected cell\n",
    "    # if i in cells_subset[0:5]:\n",
    "    #     print(f\"Displaying image for cell {i}\")\n",
    "        # plt.imshow(img_sub_norm, cmap=\"gray\")  # Uncomment this line and below to display sample cell images\n",
    "        # plt.show()\n",
    "\n",
    "    # create directory for images if it doesn't exist\n",
    "    cell_dir = os.path.join(os.getcwd(), data_dir, \"cell_images_py\")\n",
    "    if not os.path.exists(cell_dir):\n",
    "        os.mkdir(cell_dir)\n",
    "\n",
    "    # create directory for cluster if it doesn't exist\n",
    "    clust_dir = os.path.join(cell_dir, f\"cluster_{clustval_i}\")\n",
    "    if not os.path.exists(clust_dir):\n",
    "        os.mkdir(clust_dir)\n",
    "\n",
    "    # save extracted image as a png file\n",
    "    plt.imsave(os.path.join(clust_dir, f\"cell_{i}.png\"), img_sub_norm, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc390c6-93a7-4f7a-8590-a815f558c372",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcab59-5cf0-43ba-8d87-bc9ef73da3dc",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a375486-9ec4-4acc-8318-89a53f6973d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:51.407478112Z",
     "start_time": "2024-04-24T03:16:50.678411262Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "data_dir = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"data_processed\"))\n",
    "dir_path = os.path.abspath(os.path.join(script_dir, \"..\", \"..\", \"data_raw_big/Xenium_V1_FF_Mouse_Brain_MultiSection_1_outs\"))\n",
    "file_path = os.path.join(data_dir, \"morphology_focus.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed737d09-2bae-48d5-a218-f9ad833bef37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:51.941609793Z",
     "start_time": "2024-04-24T03:16:51.931643032Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = data_dir\n",
    "CELL_IMAGES = os.path.join(DATA_DIRECTORY, \"cell_images_py\")\n",
    "SIZE = (50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading in and displaying images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558bbf8c-2d5a-40d3-b546-c6497bad4572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_cluster_images(cluster: int, size=(None, None)):\n",
    "    DIR = os.path.join(CELL_IMAGES, f\"cluster_{cluster}\")\n",
    "    files = [os.path.join(DIR, file) for file in os.listdir(DIR)]\n",
    "    files.sort()\n",
    "    return [Image.open(file).convert(\"L\") if None in size else Image.open(file).convert(\"L\").resize(size) for file in files]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:54.312401623Z",
     "start_time": "2024-04-24T03:16:54.305798729Z"
    }
   },
   "id": "2a49e3bc-aa94-406a-8aeb-7b8b566a3cce",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def display_images_in_grid(images, ncols=3, cmap=None):\n",
    "    nrows = (len(images) + ncols - 1) // ncols\n",
    "    fig_height = (\n",
    "        nrows * 1\n",
    "    )  # Adjust the multiplier as needed to control spacing between rows\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, fig_height))\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        if not cmap:\n",
    "            ax.imshow(image)\n",
    "        else:\n",
    "            ax.imshow(image, cmap=cmap)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.03)  # Adjust spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#display_images_in_grid(cluster_A_images_resized, ncols=10, cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:54.619616456Z",
     "start_time": "2024-04-24T03:16:54.612556323Z"
    }
   },
   "id": "bf37099a-c206-4c16-a8cd-0d8f0de88e49",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell boundary and masking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf09db3-54b9-41a9-8754-56ba1e75e594"
  },
  {
   "cell_type": "markdown",
   "id": "ea54982f-5ef2-4fbb-8cc5-7fd8667c47d2",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5272d9-e3d9-4b38-87e1-5c47a97ae1a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:56.156249491Z",
     "start_time": "2024-04-24T03:16:56.082014857Z"
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cell_boundaries_raw = pd.read_csv(\"../../data_processed/cell_boundaries.csv.gz\")\n",
    "\n",
    "\n",
    "def get_cluster_cell_ids(cluster: int):\n",
    "    DIR = os.path.join(CELL_IMAGES, f\"cluster_{cluster}\")\n",
    "    files = [os.path.join(DIR, file) for file in os.listdir(DIR)]\n",
    "    pattern = r\"cell_(\\d+)\\.png\"\n",
    "    return [int(re.search(pattern, file).group(1)) for file in files]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:56.727832389Z",
     "start_time": "2024-04-24T03:16:56.561643130Z"
    }
   },
   "id": "e48f8441-52c6-4ab2-bd3b-1fd0ad42dabb",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "9af7c61b-987c-48b5-9774-a21d2a6dc5f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed959a4-2621-45bd-b19a-f11c8c96fbc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:57.641866906Z",
     "start_time": "2024-04-24T03:16:57.621509667Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_masked_image(\n",
    "    image: Image, cell_id: int, cell_boundaries: pd.core.frame.DataFrame\n",
    "):\n",
    "    height, width = np.array(image).shape\n",
    "    cell_boundary = cell_boundaries[cell_boundaries[\"cell_id\"] == cell_id].copy()\n",
    "    cell_boundary[\"vertex_x_scaled\"] = 1 + (\n",
    "        (cell_boundary[\"vertex_x\"] - cell_boundary[\"vertex_x\"].min()) * scaling_factor\n",
    "    )\n",
    "    cell_boundary[\"vertex_y_scaled\"] = 1 + (\n",
    "        (cell_boundary[\"vertex_y\"] - cell_boundary[\"vertex_y\"].min()) * scaling_factor\n",
    "    )\n",
    "    polygon = np.array(\n",
    "        list(zip(cell_boundary[\"vertex_y_scaled\"], cell_boundary[\"vertex_x_scaled\"]))\n",
    "    )\n",
    "    mask = polygon2mask((height, width), polygon)\n",
    "    masked_image = np.ma.masked_array(np.copy(image), np.invert(mask), fill_value=0)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c187e1a-7ccf-4f6a-9c80-e59e77a606db",
   "metadata": {},
   "source": [
    "\n",
    "### Masking all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb4d369-abb1-4efa-9597-0d92538df5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:16:58.937356190Z",
     "start_time": "2024-04-24T03:16:58.916095603Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cluster_masked_images(cluster: int, size=(None, None), masked=False):\n",
    "    images = get_cluster_images(cluster)\n",
    "    if None not in size:  # get resized images if size is specified\n",
    "        images_resized = get_cluster_images(cluster, size)\n",
    "\n",
    "    cell_ids = get_cluster_cell_ids(cluster)\n",
    "    cell_boundaries = cell_boundaries_raw.loc[\n",
    "        cell_boundaries_raw[\"cell_id\"].isin(cell_ids)\n",
    "    ].copy()\n",
    "\n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = 1 / 0.22\n",
    "\n",
    "    result = {}\n",
    "    for cell_id, image, image_resized in zip(cell_ids, images, images_resized):\n",
    "        if masked:\n",
    "            # get shape of original image (without resize)\n",
    "            height, width = np.array(image).shape\n",
    "            # scaling factor for cell boundary, only if size is given\n",
    "            resize_x = 50 / width if None not in size else 1\n",
    "            resize_y = 50 / height if None not in size else 1\n",
    "    \n",
    "            # get cell boundary coordinates for the image\n",
    "            cell_boundary = cell_boundaries.loc[\n",
    "                cell_boundaries[\"cell_id\"] == cell_id\n",
    "            ].copy()\n",
    "            # Calculate scaled x and y coordinates\n",
    "            cell_boundary[\"vertex_x_scaled\"] = (\n",
    "                1\n",
    "                + (\n",
    "                    (cell_boundary[\"vertex_x\"] - cell_boundary[\"vertex_x\"].min())\n",
    "                    * scaling_factor\n",
    "                )\n",
    "            ) * resize_x\n",
    "            cell_boundary[\"vertex_y_scaled\"] = (\n",
    "                1\n",
    "                + (\n",
    "                    (cell_boundary[\"vertex_y\"] - cell_boundary[\"vertex_y\"].min())\n",
    "                    * scaling_factor\n",
    "                )\n",
    "            ) * resize_y\n",
    "    \n",
    "            # create the mask\n",
    "            polygon = np.array(\n",
    "                list(\n",
    "                    zip(cell_boundary[\"vertex_y_scaled\"], cell_boundary[\"vertex_x_scaled\"])\n",
    "                )\n",
    "            )\n",
    "            mask = polygon2mask((height, width) if None in size else SIZE, polygon)\n",
    "            # mask the image\n",
    "            masked_image = np.ma.masked_array(\n",
    "                np.copy(image if None in size else image_resized),\n",
    "                np.invert(mask),\n",
    "                fill_value=0,\n",
    "            )\n",
    "            result[cell_id] = np.where(masked_image.mask, 0, masked_image)\n",
    "        else:\n",
    "            result[cell_id] = np.copy(image if None in size else image_resized)\n",
    " \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a76b5a-fbdd-4498-b649-7628c459af2b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:12.603530514Z",
     "start_time": "2024-04-24T03:16:59.298806273Z"
    }
   },
   "outputs": [],
   "source": [
    "scaling_factor = 1 / 0.22\n",
    "\n",
    "\n",
    "masked_cells = {\n",
    "    cluster: get_cluster_masked_images(cluster, size=SIZE, masked=True) for cluster in range(1, 29)\n",
    "}\n",
    "\n",
    "#unmasked_cells = {\n",
    "    #cluster: get_cluster_masked_images(cluster, size=SIZE, masked=False) #for cluster in range(1, 29)\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0786f32-eb49-4d41-a022-531ebe118a20",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46d307a9-1af4-4801-8860-a31d42218dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:13.402257909Z",
     "start_time": "2024-04-24T03:18:12.646408070Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7f121-7e10-4895-838a-d7bd8ca0ae52",
   "metadata": {},
   "source": [
    "### Make the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2845cfb9-6d4b-44fa-abda-b3523cb48717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:13.411457396Z",
     "start_time": "2024-04-24T03:18:13.405432840Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_dict, transform=None):\n",
    "        self.images_dict = images_dict\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for cluster, images in images_dict.items():\n",
    "            self.images.extend(images.values())\n",
    "            self.labels.extend([cluster - 1] * len(images))\n",
    "        self.transform = transform  # Transformation to apply to each image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformation\n",
    "\n",
    "        # Convert label to PyTorch tensor\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e28a67-8dcf-41e9-97ca-518682a9f047",
   "metadata": {},
   "source": [
    "#### Load data and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d64d6075-1135-42df-ab9c-5a8e0abcc704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:13.885571178Z",
     "start_time": "2024-04-24T03:18:13.409088717Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5], [0.5]\n",
    "        ),  # Example normalization, adjust as necessary\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pass these transformations when creating the dataset instance\n",
    "\n",
    "custom_dataset = CustomDataset(masked_cells,train_transforms)\n",
    "# Define the sizes of the training, validation, and testing sets\n",
    "train_size = int(0.6 * len(custom_dataset))  # 60% for training\n",
    "val_size = int(0.2 * len(custom_dataset))  # 20% for validation\n",
    "test_size = len(custom_dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    custom_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create DataLoader for testing set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf61fbe-a5fe-4fd8-a538-a174b2362ea7",
   "metadata": {},
   "source": [
    "### Make the models"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a modified Vision Transformer (ViT) with a dropout layer\n",
    "class ModifiedViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"vit_base_patch16_224\",\n",
    "        pretrained=True,\n",
    "        num_classes=1000,\n",
    "        dropout_rate=0.3,  # Default dropout rate\n",
    "    ):\n",
    "        super(ModifiedViT, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ViT model\n",
    "        self.vit = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=num_classes\n",
    "        )\n",
    "\n",
    "        # Modify the patch embedding to handle grayscale images\n",
    "        original_patch_embedding = self.vit.patch_embed.proj\n",
    "        self.vit.patch_embed.proj = nn.Conv2d(\n",
    "            1,\n",
    "            original_patch_embedding.out_channels,\n",
    "            kernel_size=original_patch_embedding.kernel_size,\n",
    "            stride=original_patch_embedding.stride,\n",
    "            padding=original_patch_embedding.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # If using pretrained weights, adjust for grayscale\n",
    "        if pretrained:\n",
    "            with torch.no_grad():\n",
    "                original_weight = original_patch_embedding.weight.mean(\n",
    "                    dim=1, keepdim=True\n",
    "                )\n",
    "                self.vit.patch_embed.proj.weight.data = original_weight\n",
    "\n",
    "        # Add dropout layer after patch embedding\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Replace the classifier head\n",
    "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit.patch_embed(x)\n",
    "\n",
    "        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Apply positional dropout and add positional embedding\n",
    "        x = self.vit.pos_drop(x + self.vit.pos_embed)\n",
    "        \n",
    "        # Apply dropout before entering transformer blocks\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Forward through the transformer blocks\n",
    "        x = self.vit.blocks(x)\n",
    "        \n",
    "        # Apply normalization\n",
    "        x = self.vit.norm(x)\n",
    "        \n",
    "        # Select CLS token and apply final dropout\n",
    "        x = self.dropout(x[:, 0])\n",
    "        \n",
    "        # Final classifier head\n",
    "        x = self.vit.head(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:07:56.147207953Z",
     "start_time": "2024-04-24T03:07:55.719509078Z"
    }
   },
   "id": "61c8b3336e3f3e1f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 6 * 6, 128\n",
    "        )  # Adjust input size based on your image size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 6 * 6)  # Adjust input size based on your image size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:07:57.197729481Z",
     "start_time": "2024-04-24T03:07:57.191217866Z"
    }
   },
   "id": "be8edf3d8254d694",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Lab model\n",
    "class LabModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(\n",
    "            64 * 12 * 12, 128\n",
    "        )  # Adjust input size based on your image size\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(64, 28)  # Assuming 2 classes for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 64 * 12 * 12)  # Adjust input size based on your image size\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:27.358573501Z",
     "start_time": "2024-04-24T03:18:27.315942193Z"
    }
   },
   "id": "32d7f8d11f64734e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the AlexNet model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num=28):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(32 * 12 * 12, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(-1, 32 * 12 * 12)\n",
    "        return self.classifier(x)\n",
    "        # return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:07:58.852408561Z",
     "start_time": "2024-04-24T03:07:58.846973243Z"
    }
   },
   "id": "acdc26998d7e0cba",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Define the ResNet model\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ModifiedResNet50(models.ResNet):\n",
    "    def __init__(self, weights=None, num_classes=28, *args, **kwargs):\n",
    "        super(ModifiedResNet50, self).__init__(\n",
    "            block=models.resnet.Bottleneck, layers=[3, 4, 6, 3], *args, **kwargs\n",
    "        )\n",
    "\n",
    "        # Initialize a new fully connected layer with the correct number of classes\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        # If weights is provided, load the state_dict with strict=False to avoid errors\n",
    "        if weights is not None:\n",
    "            state_dict = weights.state_dict()\n",
    "\n",
    "            # Remove the weights related to the fully connected layer to avoid mismatch\n",
    "            if 'fc.weight' in state_dict and 'fc.bias' in state_dict:\n",
    "                del state_dict['fc.weight']\n",
    "                del state_dict['fc.bias']\n",
    "\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Update the first layer's weights, if weights were provided\n",
    "        if weights is not None:\n",
    "            with torch.no_grad():\n",
    "                original_first_layer = state_dict[\"conv1.weight\"]\n",
    "                self.conv1.weight.data = original_first_layer.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Adding Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Adding Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:08:00.162087680Z",
     "start_time": "2024-04-24T03:08:00.153700895Z"
    }
   },
   "id": "5949f432209aa77a",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "d19ce063-a845-4a4c-b5d1-9556d0324272",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4801704e-d964-463d-b281-969d3e2d5c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:20:36.711496337Z",
     "start_time": "2024-04-24T03:20:36.668934120Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Lists to store training and validation loss and accuracy\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    train_loop = tqdm(train_loader, total=len(train_loader), leave=True, position=0)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in train_loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        epoch_train_accuracy = correct_train / total_train\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_accuracy = correct_val / total_val\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "        # Print progress\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {epoch_train_accuracy:.4f}, \"\n",
    "            f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Return training and validation losses and accuracies\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b85b188-c3fd-4766-b135-df0f4d6266b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T03:18:31.219189418Z",
     "start_time": "2024-04-24T03:18:31.179173402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1ddee75-090d-4137-a8fb-91e74953a6c5",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T03:20:41.153408545Z",
     "start_time": "2024-04-24T03:20:40.833156205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/686 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 9216]' is invalid for input of size 6422528",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m LabModel()\n\u001B[0;32m----> 9\u001B[0m t_loss, v_loss, t_acc, v_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m lab_model_results \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_loss, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: v_loss, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_acc, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m: v_acc}\n",
      "Cell \u001B[0;32mIn[17], line 37\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001B[0m\n\u001B[1;32m     34\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/myvirtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/myvirtualenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 23\u001B[0m, in \u001B[0;36mLabModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     21\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n\u001B[0;32m---> 23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Adjust input size based on your image size\u001B[39;00m\n\u001B[1;32m     24\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))\n\u001B[1;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[-1, 9216]' is invalid for input of size 6422528"
     ]
    }
   ],
   "source": [
    "from torch import device, Tensor\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Load the pre-trained ResNet model\n",
    "torch.cuda.empty_cache()\n",
    "model = LabModel()\n",
    "\n",
    "t_loss, v_loss, t_acc, v_acc = train_model(model, train_loader, val_loader,num_epochs= 30)\n",
    "lab_model_results = {\"t_loss\": t_loss, \"v_loss\": v_loss, \"t_acc\": t_acc, \"v_acc\": v_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbd713-28a2-4c60-8828-7ec93aca50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_plots(lab_model_results['t_loss'], lab_model_results['v_loss'], lab_model_results['t_acc'], lab_model_results['v_acc'])\n",
    "def generate_plots(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    with plt.style.context(\"dark_background\"):\n",
    "        # Generate x-axis values (epochs)\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        ax1.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "        ax1.plot(epochs, val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "\n",
    "        # Add jitter to the data points for better visualization\n",
    "        ax1.scatter(epochs, train_losses, color=\"blue\")\n",
    "        ax1.scatter(epochs, val_losses, color=\"orange\")\n",
    "\n",
    "        # Add labels and legend for the first subplot\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Training and Validation Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot training and validation accuracies\n",
    "        ax2.plot(epochs, train_accuracies, label=\"Training Accuracy\", marker=\"o\")\n",
    "        ax2.plot(epochs, val_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n",
    "\n",
    "        # Add jitter to the data points for better visualization\n",
    "        ax2.scatter(epochs, train_accuracies, color=\"green\")\n",
    "        ax2.scatter(epochs, val_accuracies, color=\"red\")\n",
    "\n",
    "        # Add labels and legend for the second subplot\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Training and Validation Accuracy\")\n",
    "        ax2.legend()\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e851cb-eae4-4135-9820-95a83387c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots(lab_model_results['t_loss'], lab_model_results['v_loss'], lab_model_results['t_acc'], lab_model_results['v_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
