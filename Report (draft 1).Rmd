---
title: Impacts of Transformation Techniques and Deep Learning Models on Cell Images Classification Accuracy
author:
  - name: "Image02 - 510050786, 520181814, 510652339, 510401900, 520406670, 500469361"
address:
  - code: a
    address: The University of Sydney
doi_footer: "https://github.com/jiyu4399/DATA3888_Image_Group_2"
numbersections: true
papersize: letter
fontsize: 9pt
skip_final_break: true
output: pinp::pinp
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}

```

# Executive Summary

This project aims to enhance the classification of cell images by gene expression cluster labels, a process essential for medical and biotechnical researchers navigating complex large-scale cell imaging data. 

The project found that data augmentation and masking techniques significantly improves the classification accuracy. However, more advanced and deeper networks capture the complex patterns among cell images more efficiently thus having better predicting performance.
Included in the report are key figures demonstrating our results with a bar plot on basic CNN performance with data augmentation and masking techniques, a comparative F1 score chart for ResNet18 versus basic CNN across clusters, and an accuracy graph for ViT and transformers across training epochs.

Actionable insights of the impact of data transformations and deep learning models is in the creation of an interactive Shiny platform. The application enables users to upload cell images and receive classifications based on selected models and transformation techniques, translating complex data science processes into actionable insights for users.


# Introduction 

The interpretation and classification of cell images based on gene expression cluster labels play a crucial role in the fields of science and medicine. Cells often exhibit complex phenotypes, such as variations in shape, subcellular protein localization, and other characteristics. The combination of these intricate phenotypic differences and diverse biological responses makes deciphering biological processes increasingly complex (Shifat-E-Rabbi et.al., 2020). Therefore, traditional medical research often struggles with image analysis and prediction due to technical expertise limitations, which is a problem that tends to worsen with increasing data volume and complexity (Panayides et al., 2020).

To address this issue, the use of advanced computational techniques has become more and more popular. Computer analysis of cell images can automatically classify cells, providing valuable assistance in scientific, technological, and medical applications. For instance, in pathology, researchers are increasingly exploring methods to automatically detect cancer through cell image classification (Kantara et. al., 2015). 

The aim of this project, therefore, is to investigate the impacts of different data processing techniques and deep learning models on classification performance over cell image data. Our primary focus is on leading medical and biotechnological researchers who have to process huge volumes of cell imaging data, and are particularly interested in exploring cellular localization. To assist these researchers, we introduce a computer vision scheme that accurately identifies cell identities and provides detailed insights into the impact of various data transformations and deep learning models on the classification results. This project thus seeks to help researchers gain a deeper understanding of the potential drawbacks associated with their current data collection and experimental design. By providing recommendations and insights, we hope to refine the design and efficiency of cell classification experiments, state-of-the-art methodologies from the data science field, and achieve more accurate results.

Given the complexity and pervasiveness of cell images, we decided to focus on deep learning methods, including convolutional neural networks (CNNs) and Transformers, which have emerged as the leading end-to-end classification system (Shifat-E-Rabbi et al., 2020). Our study also includes various image augmentation techniques to enhance the performance and accuracy of these models. 

Building on the key findings, an interactive platform - Shiny, is developed which will allow users to get insights of models’ performance by returning evaluation metrics and detailed description about the selected combination of model and data transformations techniques. The platform also allows users to upload new cell images and select a specific set of model and augmentation techniques to obtain predicted cluster IDs, along with corresponding predicted probability. Creating a user-friendly interface that translates complex data science processes into actionable clear insights visually like Shiny could foster deeper understanding and more effective application of cell image classification techniques.


# Methodology

## Data acquisition
The initial experimentation with the provided images using basic machine learning techniques identified overfitting and low fitting efficiency as significant issues (Appendix ). To address these, the project considered an extended training input with transformation techniques (image contour methods) and extended deep learning models. Therefore, the extended image dataset, sourced from the 10X Genomics platform, includes high-resolution cell images, cluster IDs, and cell boundary information from a 10µm section of a C57BL/6 mouse from Charles River Laboratories. The images are stored as lists, with each list representing a grayscale image composed of an array of pixel intensity values. This dataset, comprising 36,602 images across 28 clusters, was subsequently used for further analysis. 

\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.4\textwidth]{methodology.png}
    \caption{Figure 2: Methodology}
    \label{fig:methodology}
\end{figure}

## Model development
Based on the reintroduced larger dataset and the problems identified through experiments, it was evident that simple machine learning models and the basic CNN model lacked the necessary complexity. Consequently, our project narrowed its scope to focus solely on deep learning models. ResNet18 and ResNet50 were considered initially, as these mainstream classifiers learn the residual function of the reference layer input, forming a network by stacking residual blocks, their architecture is considered more suitable for optimisation. Beyond CNN models, another main class investigated was Vision Transformers (ViT), which excel in handling large datasets and offer more robust generalization. The overall deep learning models as shown:

- Basic CNN Model (CNN)
- Resnet 18 (CNN)
- Resnet 50 (CNN)
- ViT (Transformer)

Specifically, the basic CNN model we constructed consists of two convolutional layers with ReLU activation and 2x2 max pooling layers, reducing the input dimensions from 50x50 to 12x12. ResNet18 comprises 18 layers including convolutional, batch normalization, and identity mapping layers, and ResNet50 consists of 50 layers with a more complex architecture. Different from CNN models, the Vit model (Vit-base) contains 12 layers.

## Transformation and Masking

Besides efficient models, image augmentation has been proven as an effective and efficient strategy to obtain satisfactory performance with a limited dataset (Xu et al., 2023), which remains as a prevalent obstacle in medical image analysis (Shorten et al., 2029). The project thus incorporates several image  transformation and masking techniques, including data normalization and augmentation, with Gaussian filters, Canny contour detection, Sobel edge detection. 

Normalization was applied to the pixel-based data by subtracting the mean of pixel values and dividing by the standard deviation for each image, projecting the pixel values within the predefined range of [0, 1] to ensure consistency within the training input. Besides, random flipping and random rotation (image augmentation), which have been commonly utilized throughout many studies over the last decade for various computer vision tasks such as image classification (Xu et al., 2023), were also implemented with rotating angles ranging from -50 to 50 degrees. 

To further enhance model performance, Gaussian blur was implemented by convolving each image with a Gaussian function to smooth the image and reduce noise. This assigns a weight to each pixel based on its distance from the center, with the blur size set to (25, 25) to specify the kernel size. The convolution operation calculates a weighted average of the surrounding pixel values, reducing high-frequency noise. Additionally, the Canny edge detection method was applied using thresholds of 10 and 200 to detect a wide range of edges by identifying intensity gradients. The final investigation focused on the Sobel edge detector, which performs 2-D spatial gradient measurements on the image to emphasize high spatial frequency regions corresponding to edges.
To evaluate the effectiveness of the two transformation, three masking techniques, alongwith the determination of the optimal combination with different models, we established the Basic CNN model as our baseline. This decision was made in consideration of the limited computational resources available to the project team.

## Training and Validation

The training and validation process began by experimenting with different combinations of transformation techniques on the dataset, which was initially split into 80% training and 20% testing sets. For all of the CNN models, based on 4-fold cross-validation, the training set was further divided into four folds, each containing 20% of the data, and models were trained using these folds. Considering the limitation of computational resources, the ViT model, along with the optimal CNN model, was trained and validated with a single validation set.

During training, instances of overfitting were observed, which were addressed by adding dropout layers to each model. Additionally, the performance of three optimizers—SGD, BGD, and ADAM—was compared, where SGD exhibited noticeable fluctuations, BGD had slow convergence, and ADAM, being relatively robust, was ultimately chosen for all models. Upon completing the training process, the final model's performance was evaluated using the untouched testing set.

## Evaluation strategy

Our evaluation strategies to gauge model performance measured both per-cluster and overall levels. Accuracy serves as the primary indicator of model success, while precision and recall provide more nuanced insights into the model’s predictive capabilities, particularly in class imbalances. Precision indicates the proportion of true positive predictions out of all positive predictions made by the model, while recall measures the proportion of true positive cases detected out of all actual positive cases. Generally, precision and recall are inversely related: a more conservative model might have higher precision but lower recall, and vice versa.

To address this, we calculated the F1 score, which balances both precision and recall to provide a single metric that considers both false positives and false negatives. However, we also report precision and recall separately to allow for more specific evaluation scenarios.
Furthermore, the confusion matrix further measures model performance by revealing possible patterns of misclassification. To ensure scalability and feasibility for real-world applications, the models are also evaluated in terms of running time and memory usage. 





# Result

## Convolutional Neural Networks (CNNs)

### Basic CNN model


```{r,echo=FALSE}
library(jsonlite)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)

# Function to load metrics from JSON file
load_metrics <- function(json_filepath) {
  tryCatch({
    data <- fromJSON(json_filepath)
    return(data)
  }, error = function(e) {
    return(NULL)
  })
}

# Function to generate a large color palette
generate_colors <- function(n) {
  if (n <= 9) {
    return(brewer.pal(n, "Set1"))
  } else {
    return(colorRampPalette(brewer.pal(9, "Set1"))(n))
  }
}

load_and_prepare_metrics <- function(folder_path, num_folds = 5, num_repeats = 3) {
  all_conf_matrices <- list()
  all_precision <- list()
  all_recall <- list()
  all_f1_scores <- list()
  all_accuracies <- list()  # To store overall accuracy values
  all_epoch_accuracies <- list()  # To store epoch-wise accuracy values
  
  for (fold in 1:num_folds) {
    for (repeat_idx in 1:num_repeats) {
      json_filepath <- file.path(folder_path, paste0("Repeat_", repeat_idx, "_Fold_", fold, "/evaluation_metrics.json"))
      data <- load_metrics(json_filepath)
      if (!is.null(data)) {
        all_conf_matrices[[length(all_conf_matrices) + 1]] <- as.matrix(data$confusion_matrix)
        all_precision[[length(all_precision) + 1]] <- data$`Precision by cluster`
        all_recall[[length(all_recall) + 1]] <- data$`Recall by cluster`
        all_f1_scores[[length(all_f1_scores) + 1]] <- data$`F1 score by cluster`
        all_accuracies[[length(all_accuracies) + 1]] <- data$Accuracy  # Extract overall accuracy
        all_epoch_accuracies[[length(all_epoch_accuracies) + 1]] <- data$epoch_accuracies[[1]]  # Extract epoch-wise accuracies
      }
    }
  }
  
  results <- list()
  if (length(all_conf_matrices) > 0) {
    results$avg_conf_matrix <- Reduce("+", all_conf_matrices) / length(all_conf_matrices)
  } else {
    results$avg_conf_matrix <- NULL
  }
  
  if (length(all_precision) > 0) {
    all_precision <- do.call(rbind, all_precision)
    results$avg_precision <- colMeans(all_precision)
  } else {
    results$avg_precision <- NULL
  }
  
  if (length(all_recall) > 0) {
    all_recall <- do.call(rbind, all_recall)
    results$avg_recall <- colMeans(all_recall)
  } else {
    results$avg_recall <- NULL
  }
  
  if (length(all_f1_scores) > 0) {
    all_f1_scores <- do.call(rbind, all_f1_scores)
    results$avg_f1_scores <- colMeans(all_f1_scores)
  } else {
    results$avg_f1_scores <- NULL
  }
  
  if (length(all_accuracies) > 0) {
    results$avg_accuracy <- mean(unlist(all_accuracies))  # Calculate average overall accuracy
  } else {
    results$avg_accuracy <- NULL
  }
  
  if (length(all_epoch_accuracies) > 0) {
    results$avg_epoch_accuracies <- colMeans(do.call(rbind, all_epoch_accuracies))  # Calculate average epoch-wise accuracies
  } else {
    results$avg_epoch_accuracies <- NULL
  }
  
  return(results)
}


# Define directories and load metrics
directories <- c(
  "shiny/LabModel_n_nm",
  "shiny/LabModel_nrf_nm",
  "shiny/LabModel_nrfrr_cb",
  "shiny/LabModel_nrfrr_gau",
  "shiny/LabModel_nrfrr_nm",
  "shiny/LabModel_nrfrr_se",
  "shiny/LabModel_nt_nm",
  "shiny/ResNet18_nrfrr_cb",
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet18_nrfrr_nm",
  "shiny/ResNet18_nrfrr_se",
  "shiny/ResNet50_nrfrr_cb",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/ResNet50_nrfrr_nm",
  "shiny/ResNet50_nrfrr_se"
)
results_list <- lapply(directories, function(dir) {
  load_and_prepare_metrics(dir, num_folds = 4, num_repeats = 1)
})

# Function to generate a custom color palette for models
generate_custom_colors <- function() {
  custom_colors <- c("#1f77b4", "#ff7f0e", "#2ca02c")  # Custom colors for ResNet18, ResNet50, and Simple CNN
  return(custom_colors)
}

# Filter and rename specified models
filter_and_rename_models <- function(results_list, directories, target_models, new_names) {
  f1_data <- data.frame()
  
  for (i in 1:length(results_list)) {
    model_name <- directories[i]
    if (model_name %in% target_models) {
      new_name <- new_names[target_models == model_name]
      f1_scores <- results_list[[i]]$avg_f1_scores
      if (!is.null(f1_scores)) {
        temp_df <- data.frame(
          Model = new_name,
          Cluster = 1:length(f1_scores),
          F1_Score = f1_scores
        )
        f1_data <- rbind(f1_data, temp_df)
      }
    }
  }
  
  return(f1_data)
}

# Define target models and new names
target_models <- c(
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/LabModel_nrfrr_gau"
)
new_names <- c("ResNet18", "ResNet50", "Simple CNN")

# Generate the filtered and renamed F1 scores data frame
f1_data <- filter_and_rename_models(results_list, directories, target_models, new_names)

# Remove the cluster for mean values if it exists (usually the last cluster)
f1_data <- f1_data[f1_data$Cluster <= max(f1_data$Cluster) - 1, ]

# Correct technique extraction for multiple masking techniques
prepare_accuracy_data <- function(results_list, directories, target_models, new_names) {
  accuracy_data <- data.frame()
  
  for (i in 1:length(results_list)) {
    model_name <- directories[i]
    if (model_name %in% target_models) {
      new_name <- new_names[target_models == model_name]
      overall_accuracy <- results_list[[i]]$avg_accuracy
      if (!is.null(overall_accuracy)) {
        technique <- sub(".*nrfrr_([a-z]+)", "\\1", model_name)
        technique <- switch(technique,
                            cb = "Basic Boundary",
                            gau = "Gaussian Blur",
                            nm = "No Mask",
                            se = "Sobel",
                            technique)  # Handle cases that don't match
        temp_df <- data.frame(
          Model = new_name,
          Technique = technique,
          Accuracy = overall_accuracy  # Use the overall accuracy
        )
        accuracy_data <- rbind(accuracy_data, temp_df)
      }
    }
  }
  
  return(accuracy_data)
}

# Define the target models and new names
target_models <- c(
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/LabModel_nrfrr_gau",
  "shiny/ResNet18_nrfrr_cb",
  "shiny/ResNet50_nrfrr_cb",
  "shiny/LabModel_nrfrr_cb",
  "shiny/ResNet18_nrfrr_nm",
  "shiny/ResNet50_nrfrr_nm",
  "shiny/LabModel_nrfrr_nm",
  "shiny/ResNet18_nrfrr_se",
  "shiny/ResNet50_nrfrr_se",
  "shiny/LabModel_nrfrr_se"
)
new_names <- c(
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN"
)


```

```{r,echo=FALSE,fig.height=2.5,fig.width=2,fig.cap="Models accuracy under different masking technologies"}
# Generate the accuracy data frame with updated techniques
accuracy_data <- prepare_accuracy_data(results_list, directories, target_models, new_names)

# Reorder the techniques and models
accuracy_data$Technique <- factor(accuracy_data$Technique, levels = c("No Mask", "Basic Boundary", "Sobel", "Gaussian Blur"))
accuracy_data$Model <- factor(accuracy_data$Model, levels = c("Simple CNN", "ResNet50", "ResNet18"))

# Create the horizontal bar plot for accuracy distribution by masking technique
p_acc <- ggplot(accuracy_data, aes(x = Technique, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +  # Use geom_bar for bar plot
  coord_flip() +  # Rotate the plot
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.20)) +  # Set y-axis as percentage with upper limit 20%
  labs(
    x = "Masking Technique",
    y = "Accuracy",
    fill = "Model"
  ) +
  theme_minimal(base_size = 8) +  # Adjust base size for better readability
  theme(
    axis.title.x = element_text(margin = margin(t = 6), size = 8),
    axis.title.y = element_text(margin = margin(r = 6), size = 8),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7),
    legend.position = "bottom",
    legend.title = element_text(face = "bold", size = 8),
    legend.text = element_text(size = 7),
    legend.box = "vertical",
    legend.box.just = "left",
    legend.direction = "horizontal",
    legend.key.size = unit(0.5, "lines"),
    strip.background = element_rect(fill = "lightblue", color = "black"),
    strip.text = element_text(face = "bold", size = 8),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank(),
    plot.margin = margin(10, 10, 10, 10)  # Adjust plot margins
  ) +
  guides(fill = guide_legend(nrow = 3, byrow = TRUE))  # Arrange legend in three rows

print(p_acc)

```


Applying image augmentation techniques yields noticeable improvements in the performance of the basic CNN model. Specifically, using the transformed dataset increases 2.6% of accuracy, 3% of precision, and 1 of F1 score than the original one. Among that, normalization accounted for most of the results, noticeably doubling the precision. Given the fact that normalization reduces the differences between the variation range of the different variables, error gradually decreases (Sola et al., 1997) which improves both convergence and generalization of the network (Shao et al., 2020). 
Image augmentation techniques (random rotation and random flip), except for increasing the accuracy 0.9%, has no impact on the precision and even decreases the F1 score by 1. As these cells in medical images can appear in many variations, these geometric transformations can introduce this variability into the model by changing the perspectives of objects within images (Xu et al., 2023), mitigating the overfitting issue encountered initially. However, as both the angle of rotation and axis of flipping directly affect the preservation label identity and variations in the dataset (Xu et al., 2023), the unfavorable outcome might indicate that the chosen magnitude of these operations within this project is not the most optimal one.


All the masking techniques, meanwhile, appear to add no advance to the model performance. They even worsen the model’s accuracy by 0.5 - 0.7%. Although those edge detection and blur type identification techniques have been proven to have remarkable performance on deep learning models (Wang et al., 2017; Guo et al., 2021), the simple architecture of  the basic CNN causes its limited capacity and sensitivity to input variations of architecture, which might lead to less capability of benefiting from such complex image preprocessing techniques like masking. The lack of the improvement could indicate that the basic CNN model’s architecture is not sophisticated enough to capitalise on the increased variability without additional modification (Tuggener, L. et al 2022).

### Advanced CNN models (ResNet models)

Applying masking techniques on more advanced CNN models shows contrary results, significantly improving the models’ accuracy. Specifically, including the cell boundary increases the accuracy by 3.3% on ResNet18 and 2.2% on ResNet50. Using sobel edge adds 4.3% more on the accuracy of ResNet18, and 0.4% for ResNet50. Gaussian blur, which is also the masking technique with best performance on advanced CNNs, better the ResNet18’s accuracy by 6.4% and ResNet50 by 5.16% (Appendix x). Given the deeply sophisticated architectures, these two networks appear to gain significant advantages from these advanced image preprocessing methods: masking to better their performance.


### Overall CNN models
Comparing the performance of three CNN models over 30 epochs based on the optimal combination (Appendix), the basic CNN model achieved a validation accuracy of 15%, with both training and validation curves remaining smooth. ResNet18 reached a validation accuracy of 17%, stabilizing after initial fluctuations, while ResNet50 achieved approximately 14.5%. None of the models exhibited overfitting, indicating the effectiveness of our preprocessing methods. 

Further comparing the F1 scores at the per-cluster level (Figure), the ResNet18 model demonstrates superior performance with an average F1 score of 0.09, better than the average F1 score for ResNet50 (0.07) and basic CNN model (0.05). This indicates that, on average, the ResNet18 balances precision and recall better than others. Specifically, for Cluster 6, ResNet18 achieves an F1 score of 0.45, which matches the performance of the basic CNN. But ResNet18 also outperforms the basic CNN in several other clusters (clusters 2 and 3), by achieving higher F1 scores, indicating its robustness and better generalization capabilities compared to the basic CNN.


```{r,echo=FALSE,fig.width=2,fig.height=4,fig.cap="Average F1 Scores by Cluster for Selected Models"}
# Visualization of F1 scores using ggplot2 with facets and horizontal bars
p <- ggplot(f1_data, aes(x = as.factor(Cluster), y = F1_Score, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(limits = c(0, 0.7)) +  # Set y-axis limit from 0 to 0.7
  labs(
    x = "Cluster",
    y = "Average F1 Score",
    fill = "Model"
  ) +
  facet_wrap(~ Model, ncol = 1, scales = "free_y") +  # Facet by Model with one column
  theme_minimal(base_size = 5.6) +  # Reduced base size by 20%
  theme(
    axis.title.x = element_text(margin = margin(t = 10), size = 5.6),
    axis.title.y = element_text(margin = margin(r = 10), size = 5.6),
    legend.position = "bottom",
    legend.title = element_text(face = "bold", size = 5.6),
    legend.text = element_text(size = 5),
    strip.background = element_rect(fill = "lightblue", color = "black", linewidth = 0.5),
    strip.text = element_text(face = "bold", size = 5.6),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_line(color = "grey90"),
    panel.spacing = unit(1, "lines")  # Increase the space between panels
  )

print(p)


```




```{r,echo=FALSE,fig.height=1.5,fig.width=2.5,fig.cap="F1 Score Distribution by Model",warning=FALSE}

# Create the boxplot with jitter
p_box <- ggplot(f1_data, aes(x = Model, y = F1_Score, fill = Model)) +
  geom_boxplot(width = 0.4, outlier.size = 0.3) +  # Increase the width of the boxplot and set outlier size
  geom_jitter(shape = 16, position = position_jitter(width = 0.2), size = 0.2, alpha = 0.4) +  # Further adjust jitter points
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(limits = c(0, 0.7)) +  # Set y-axis limit from 0 to 0.7
  labs(
    x = "Model",
    y = "F1 Score",
    fill = "Model"
  ) +
  theme_minimal(base_size = 5.6) +  # Adjust base size for better readability
  theme(
    axis.title.x = element_text(margin = margin(t = 10), size = 5.6),
    axis.title.y = element_text(margin = margin(r = 10), size = 5.6),
    legend.position = "none",
    strip.background = element_rect(fill = "lightblue", color = "black", linewidth = 0.5),
    strip.text = element_text(face = "bold", size = 5.6),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_line(color = "grey90")
  )

print(p_box)

```

Thus, overall, the ResNes18 would be the best model, and as ResNet50 and basic CNN have the similar accuracy but better F1-score, we suggest ResNet50 still outperforms basic CNN.

The good performance of ResNet18 can be attributed to its more complex architecture, which includes additional convolutional and pooling operations (Ramzan et.al., 2019), allowing it to capture more intricate features compared to the basic CNN model. 

Interestingly, ResNet50 did not outperform the basic CNN with its accuracy but the F1-score, by observing the F1 distribution, while the basic CNN model shows a higher F1 score for certain clusters, both ResNet provide more comprehensive predictions across all clusters. We believed it could be attributed to the observed imbalance in the data, where the basic CNN model’s tendency to over-predict certain clusters likely contributed to its seemingly better performance in those specific cases. In addition to this, as the ResNet50 is more complex with the architecture, the lack of performance in ResNet50 compared to ResNet18 might still be due to the model's tendency to over-predict certain clusters for ResNet18. The imbalance issue also leads to its comparable or even lower performance.

## Transformers (ViT)

Under the single validation training, the accuracy of the ViT model increased steadily and continued to grow, while the validation accuracy began to stabilize at around 17.5% after 10 epochs (Figure). In the latter half of the training, we observed signs of overfitting, but the validation curve did not decline significantly, it was considered as a "good" overfitting.

Figure 3: Train and validation accuracy between ViT and ResNet18

Comparing the single validation training of the best CNN model (ResNet18), ViT outperformed the CNN by approximately 4% in accuracy. Additionally, ViT made predictions on more clusters (wider prediction) and exhibited higher precision and recall both for each cluster and overall (Appendix). Therefore, it could be concluded that the ViT model simultaneously satisfies a wide range of predictions and recalls, and demonstrates better performance. This improved performance can be attributed to the generalization capabilities, the ability to capture global context, combined with its extensive pretraining on large datasets. 



## Shiny 

Key findings are then visualized and presented using an interactive platform Shiny, which enables a more engaging and informative approach to the end-users. The app has three main functions, including: 

1. Providing insights of CNNs performance (accuracy, precision, F1 score, etc.). Considering that the dataset has a huge class imbalance problem, Shiny also displays models’ performance both overall and across 28 clusters.

2. Providing insights of the Transformers (ViT), with detailed comparison with the best model from CNN class: ResNet18.

3. Predicting clusterID and corresponding predicted probability of cell image uploaded by users, using selected combination of model and data transformation, masking techniques.



## Discussion and Conclusion 

The project findings highlight the importance of selecting appropriate data transformations and model architectures for the task of cell image classification. Normalisation was an universally beneficial preprocessing step, while the effectiveness of image augmentation and masking techniques varied depending on model complexity. The basic CNN model’s limitations were highlighted by its inability to benefit from more complex preprocessing methods, whereas advanced CNN models demonstrated significant improvements when these techniques were applied. Overall, ViT appears as the best model with highest prediction performance.

However, it is critical to acknowledge that as the success of deep learning in vision depends on three factors: high capacity models, computational power, and availability of large-scale labeled data (Sun et al., 2017), the project’s scope and methodology still poses several limitations. Specifically, the small dataset size constrained the complexity of the models that could be trained, as more sophisticated models require larger datasets to be effective. The significant imbalance issue across clusters might also potentially introduce certain biases into the models. Additionally, the vague nature of the cluster labels posed a challenge for model interpretability, making it difficult to understand the basis for the model’s predictions. Other shortcomings, such as the low model complexity due to limited computing resources and the lack of interpretability, highlight areas for future work to improve the methodology approach. 

Future work, therefore, can leverage more data from underrepresented classes to address the class imbalance issues such as a dataset including the full brain to provide a richer and more varied set of images for analysis. More advanced and deeper network architectures should also be considered for better prediction given the complexity of medical visuals such as cell images. Given that ViT outperforms all CNNs being considered, more networks from this class can be trained to investigate the efficiency of Transformers in medical image analysis. Additionally, the application of cross-validation (CV) across all models would ensure a more robust evaluation of their performance. 



\newpage
\section*{Appendix}
\section*{Appendix A-Correlation Matrix}
Contribution Table

\section*{Appendix B -  Forward Backward model}
<!-- \begin{figure}[h] -->
<!--     \raggedleft -->
<!--     \includegraphics[width=0.3\textwidth]{images/ForwardBackwardtable.png} -->
<!-- \end{figure} -->

\section*{Appendix C -  QQ Plot}
<!-- \begin{figure}[h] -->
<!--     \raggedleft -->
<!--     \includegraphics[width=0.3\textwidth]{images/QQ.jpg} -->
<!-- \end{figure} -->

