---
title: Impacts of Transformation Techniques and Deep Learning Models on Cell Images Classification Accuracy
author:
  - name: "Image02 - 510050786, 520181814, 510652339, 510401900, 520406670, 500469361"
address:
  - code: a
    address: The University of Sydney
doi_footer: "https://github.com/jiyu4399/DATA3888_Image_Group_2"
numbersections: true
papersize: letter
fontsize: 9pt
skip_final_break: true
bibliography: report.bib
output: pinp::pinp
editor_options: 
  markdown: 
    wrap: 72
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 3.6, fig.height = 6)
library(jsonlite)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(RColorBrewer)
library(ggpubr)
library(tidyverse)
```

```{r, echo=FALSE}

# Function to load metrics from JSON file
load_metrics <- function(json_filepath) {
  tryCatch({
    data <- fromJSON(json_filepath)
    return(data)
  }, error = function(e) {
    return(NULL)
  })
}

# Function to generate a large color palette
generate_colors <- function(n) {
  if (n <= 9) {
    return(brewer.pal(n, "Set1"))
  } else {
    return(colorRampPalette(brewer.pal(9, "Set1"))(n))
  }
}

load_and_prepare_metrics <- function(folder_path, num_folds = 5, num_repeats = 3) {
  all_conf_matrices <- list()
  all_precision <- list()
  all_recall <- list()
  all_f1_scores <- list()
  all_accuracies <- list()  # To store overall accuracy values
  all_epoch_accuracies <- list()  # To store epoch-wise accuracy values
  
  for (fold in 1:num_folds) {
    for (repeat_idx in 1:num_repeats) {
      json_filepath <- file.path(folder_path, paste0("Repeat_", repeat_idx, "_Fold_", fold, "/evaluation_metrics.json"))
      data <- load_metrics(json_filepath)
      if (!is.null(data)) {
        all_conf_matrices[[length(all_conf_matrices) + 1]] <- as.matrix(data$confusion_matrix)
        all_precision[[length(all_precision) + 1]] <- data$`Precision by cluster`
        all_recall[[length(all_recall) + 1]] <- data$`Recall by cluster`
        all_f1_scores[[length(all_f1_scores) + 1]] <- data$`F1 score by cluster`
        all_accuracies[[length(all_accuracies) + 1]] <- data$Accuracy  # Extract overall accuracy
        all_epoch_accuracies[[length(all_epoch_accuracies) + 1]] <- data$epoch_accuracies[[1]]  # Extract epoch-wise accuracies
      }
    }
  }
  
  results <- list()
  if (length(all_conf_matrices) > 0) {
    results$avg_conf_matrix <- Reduce("+", all_conf_matrices) / length(all_conf_matrices)
  } else {
    results$avg_conf_matrix <- NULL
  }
  
  if (length(all_precision) > 0) {
    all_precision <- do.call(rbind, all_precision)
    results$avg_precision <- colMeans(all_precision)
  } else {
    results$avg_precision <- NULL
  }
  
  if (length(all_recall) > 0) {
    all_recall <- do.call(rbind, all_recall)
    results$avg_recall <- colMeans(all_recall)
  } else {
    results$avg_recall <- NULL
  }
  
  if (length(all_f1_scores) > 0) {
    all_f1_scores <- do.call(rbind, all_f1_scores)
    results$avg_f1_scores <- colMeans(all_f1_scores)
  } else {
    results$avg_f1_scores <- NULL
  }
  
  if (length(all_accuracies) > 0) {
    results$avg_accuracy <- mean(unlist(all_accuracies))  # Calculate average overall accuracy
  } else {
    results$avg_accuracy <- NULL
  }
  
  if (length(all_epoch_accuracies) > 0) {
    results$avg_epoch_accuracies <- colMeans(do.call(rbind, all_epoch_accuracies))  # Calculate average epoch-wise accuracies
  } else {
    results$avg_epoch_accuracies <- NULL
  }
  
  return(results)
}


# Define directories and load metrics
directories <- c(
  "shiny/LabModel_n_nm",
  "shiny/LabModel_nrf_nm",
  "shiny/LabModel_nrfrr_cb",
  "shiny/LabModel_nrfrr_gau",
  "shiny/LabModel_nrfrr_nm",
  "shiny/LabModel_nrfrr_se",
  "shiny/LabModel_nt_nm",
  "shiny/ResNet18_nrfrr_cb",
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet18_nrfrr_nm",
  "shiny/ResNet18_nrfrr_se",
  "shiny/ResNet50_nrfrr_cb",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/ResNet50_nrfrr_nm",
  "shiny/ResNet50_nrfrr_se"
)
results_list <- lapply(directories, function(dir) {
  load_and_prepare_metrics(dir, num_folds = 4, num_repeats = 1)
})

# Function to generate a custom color palette for models
generate_custom_colors <- function() {
  custom_colors <- c('#A56CC1', '#A6ACEC', '#63F5EF')  # Custom colors for ResNet18, ResNet50, and Simple CNN
  return(custom_colors)
}

# Filter and rename specified models
filter_and_rename_models <- function(results_list, directories, target_models, new_names) {
  f1_data <- data.frame()
  
  for (i in 1:length(results_list)) {
    model_name <- directories[i]
    if (model_name %in% target_models) {
      new_name <- new_names[target_models == model_name]
      f1_scores <- results_list[[i]]$avg_f1_scores
      if (!is.null(f1_scores)) {
        temp_df <- data.frame(
          Model = new_name,
          Cluster = 1:length(f1_scores),
          F1_Score = f1_scores
        )
        f1_data <- rbind(f1_data, temp_df)
      }
    }
  }
  
  return(f1_data)
}

# Define target models and new names
target_models <- c(
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/LabModel_nrfrr_gau"
)
new_names <- c("ResNet18", "ResNet50", "Simple CNN")

# Generate the filtered and renamed F1 scores data frame
f1_data <- filter_and_rename_models(results_list, directories, target_models, new_names)

# Remove the cluster for mean values if it exists (usually the last cluster)
f1_data <- f1_data[f1_data$Cluster <= max(f1_data$Cluster) - 1, ]

# Correct technique extraction for multiple masking techniques
prepare_accuracy_data <- function(results_list, directories, target_models, new_names) {
  accuracy_data <- data.frame()
  
  for (i in 1:length(results_list)) {
    model_name <- directories[i]
    if (model_name %in% target_models) {
      new_name <- new_names[target_models == model_name]
      overall_accuracy <- results_list[[i]]$avg_accuracy
      if (!is.null(overall_accuracy)) {
        technique <- sub(".*nrfrr_([a-z]+)", "\\1", model_name)
        technique <- switch(technique,
                            cb = "Basic Boundary",
                            gau = "Gaussian Blur",
                            nm = "No Mask",
                            se = "Sobel",
                            technique)  # Handle cases that don't match
        temp_df <- data.frame(
          Model = new_name,
          Technique = technique,
          Accuracy = overall_accuracy  # Use the overall accuracy
        )
        accuracy_data <- rbind(accuracy_data, temp_df)
      }
    }
  }
  
  return(accuracy_data)
}

# Define the target models and new names
target_models <- c(
  "shiny/ResNet18_nrfrr_gau",
  "shiny/ResNet50_nrfrr_gau",
  "shiny/LabModel_nrfrr_gau",
  "shiny/ResNet18_nrfrr_cb",
  "shiny/ResNet50_nrfrr_cb",
  "shiny/LabModel_nrfrr_cb",
  "shiny/ResNet18_nrfrr_nm",
  "shiny/ResNet50_nrfrr_nm",
  "shiny/LabModel_nrfrr_nm",
  "shiny/ResNet18_nrfrr_se",
  "shiny/ResNet50_nrfrr_se",
  "shiny/LabModel_nrfrr_se"
)
new_names <- c(
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN",
  "ResNet18", "ResNet50", "Simple CNN"
)


```

# Executive Summary

Medical researchers often do not have supporting data to help classify
complex large-scale cell images, hence we aim to enhance classification
of cell images by gene expression cluster labels, without need for
supporting datasets.

The project found that data augmentation and masking techniques
significantly improves the classification accuracy. However, more
advanced and deeper networks capture the complex patterns among cell
images more efficiently thus having better predicting performance.

Within the report are key figures demonstrating our results with a bar
plot on basic CNN performance with data augmentation and masking
techniques, a comparative F1 score chart for ResNet18 versus basic CNN
across clusters, and an accuracy graph for VIT and transformers across
training epochs.

Actionable insights of the impact of data transformations and deep
learning models is in the creation of an interactive Shiny platform. The
application enables users, likely medical researchers, to upload cell
images and receive classifications based on selected models and
transformation techniques, translating complex data science processes
into actionable insights for users.

# Introduction

The interpretation and classification of cell images based on gene
expression cluster labels play a crucial role in the fields of science
and medicine. Cells often exhibit complex phenotypes, such as variations
in shape, subcellular protein localization, and other characteristics.
The combination of these intricate phenotypic differences and diverse
biological responses makes deciphering biological processes increasingly
complex [@ShifatERabbi2019CellIC]. Therefore, traditional medical
research often struggles with image analysis and prediction due to
technical expertise limitations, which is a problem that tends to worsen
with increasing data volume and complexity [@panayides2020AI]. To
address this issue, the use of advanced computational techniques has
become increasingly popular. Computer analysis of cell images can
automatically classify cells, providing significant support in
scientific, technological, and medical applications. For instance, in
pathology, researchers are increasingly exploring methods to
automatically detect cancer through cell image classification
[@6fd805a257464dcc92460b6cf9dcee63]. The aim of this project, therefore,
is to investigate the impacts of different data processing techniques
and deep learning models on classification performance over cell image
data. Our primary focus is on leading medical and biotechnological
researchers who need to process large volumes of cell imaging data, and
are particularly interested in exploring cellular localisation. To
assist these researchers, we introduce a computer vision scheme that
accurately identifies cell identities and provides detailed insights
into the impact of various data transformations and deep learning models
on the classification results **(Figure 1)**. This project thus seeks to help
researchers gain a deeper understanding of the potential drawbacks
associated with their current data collection and experimental design.
By providing recommendations and insights, we hope to refine the design
and efficiency of cell classification experiments, state-of-the-art
methodologies from the data science field, and achieve more accurate
results. Given the complexity and pervasiveness of cell images, we
decided to focus on deep learning methods, including convolutional
neural networks (CNNs) and Transformers, which have emerged as the
leading end-to-end classification system [@ShifatERabbi2019CellIC]. Our
study also includes various image augmentation techniques to enhance the
performance and accuracy of these models. Building on the key
findings,we developed an interactive platform - Shiny which will allow
users to get insights of models' performance by returning evaluation
metrics and detailed description about their selected combination of
model and data transformations techniques. The platform also allows
users to upload new cell images and select a specific set of model and
augmentation techniques to obtain predicted cluster IDs, along with
corresponding predicted probability. In creating a user-friendly
interface that translates complex data science processes into actionable
clear insights visually like Shiny could foster deeper understanding and
more effective application of cell image classification techniques.

```{=tex}
\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.45\textwidth]{Project workflow.png}
    \caption{Workflow}
    \label{fig:Workflow}
\end{figure}
```
# Methodology

## Data acquisition

The initial experimentation with the provided images using basic machine
learning techniques identified overfitting and low fitting efficiency as
significant issues (Appendix ). To address these, the project considered
an extended training input with transformation techniques (image contour
methods) and extended deep learning models. Therefore, the extended
image dataset, sourced from the 10X Genomics platform, includes
high-resolution cell images, cluster IDs, and cell boundary information
from a 10µm section of a C57BL/6 mouse from Charles River Laboratories.
The images are stored as lists, with each list representing a grayscale
image composed of an array of pixel intensity values. This dataset,
comprising 36,602 images across 28 clusters, was subsequently used for
further analysis.

## Model development

Based on the reintroduced larger dataset and the problems identified
through experiments, it was evident that simple machine learning models
and the basic CNN model lacked the necessary complexity. Consequently,
our project narrowed its scope to focus solely on deep learning models.
ResNet18 and ResNet50 were considered initially, as these mainstream
classifiers learn the residual function of the reference layer input,
forming a network by stacking residual blocks, their architecture is
considered more suitable for optimisation. Beyond CNN models, another
main class investigated was Vision Transformers (ViT), which excel in
handling large datasets and offer more robust generalisation. The
overall deep learning models as shown:

-   Basic CNN Model (CNN)
-   Resnet 18 (CNN)
-   Resnet 50 (CNN)
-   ViT (Transformer)

Specifically, the basic CNN model we constructed consists of two
convolutional layers with ReLU activation and 2x2 max pooling layers,
reducing the input dimensions from 50x50 to 12x12. ResNet18 comprises 18
layers including convolutional, batch normalisation, and identity
mapping layers, and ResNet50 consists of 50 layers with a more complex
architecture. Different from CNN models, the Vit model (Vit-base)
contains 12 layers.

## Transformation and Masking

In addition to efficient models, image augmentation has been proven as
an effective and efficient strategy to obtain satisfactory performance
with a limited dataset [@xu2023comprehensive], which remains as a
prevalent obstacle in medical image analysis [@shorten2019survey]. The
project thus incorporates several image transformation and masking
techniques, including data normalisation and augmentation, with Gaussian
filters, Canny contour detection, Sobel edge detection.

Normalisation was applied to the pixel-based data by subtracting the
mean of pixel values and dividing by the standard deviation for each
image, projecting the pixel values within the predefined range of [0, 1]
to ensure consistency within the training input. Image augmentations
such as random flipping and random rotation, have been commonly utilised
throughout many studies over the last decade for various computer vision
tasks such as image classification [@xu2023comprehensive], were also
implemented with rotating angles ranging from -50 to 50 degrees.

To further enhance model performance, Gaussian blur was implemented by
convolving each image with a Gaussian function to smooth the image and
reduce noise. This assigns a weight to each pixel based on its distance
from the centre, with the blur size set to (25, 25) to specify the
kernel size. The convolution operation calculates a weighted average of
the surrounding pixel values, reducing high-frequency noise.
Additionally, the Canny edge detection method was applied using
thresholds of 10 and 200 to detect a wide range of edges by identifying
intensity gradients. The final investigation focused on the Sobel edge
detector, which performs 2-D spatial gradient measurements on the image
to emphasise high spatial frequency regions corresponding to edges. To
evaluate the effectiveness of the two transformation, three masking
techniques, as well as to identify the most optimal combination with
different models, we established the Basic CNN model as our baseline.
This decision was made with consideration of the limited computational
resources available to the project team.

```{=tex}
\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.45\textwidth]{methodology.png}
    \caption{Methodology}
    \label{fig:methodology}
\end{figure}
```
## Training and Validation

The training and validation process began by experimenting with
different combinations of transformation techniques on the dataset,
which was initially split into 80% training and 20% testing sets. For
all of the CNN models, based on 4-fold cross-validation, the training
set was further divided into four folds, each containing 20% of the
data, and models were trained using these folds. Considering the
limitation of computational resources, the ViT model, along with the
optimal CNN model, was trained and validated with a single validation
set.

During training, instances of overfitting were observed, which were
addressed by adding dropout layers to each model. Additionally, the
performance of three optimizers---SGD, BGD, and ADAM---was compared,
where SGD exhibited noticeable fluctuations, BGD had slow convergence,
and ADAM, being relatively robust, was ultimately chosen for all models.
Upon completing the training process, the final model's performance was
evaluated using the untouched testing set.

## Evaluation strategy

Our evaluation strategies to gauge model performance measured both
per-cluster and overall levels. Accuracy serves as the primary indicator
of model success, while precision and recall provide more nuanced
insights into the model's predictive capabilities, particularly in class
imbalances. Precision indicates the proportion of true positive
predictions out of all positive predictions made by the model, while
recall measures the proportion of true positive cases detected out of
all actual positive cases. Generally, precision and recall are inversely
related: a more conservative model might have higher precision but lower
recall, and vice versa.

To address this, we calculated the F1 score, which balances both
precision and recall to provide a single metric that considers both
false positives and false negatives. However, we also report precision
and recall separately to allow for more specific evaluation scenarios.
Furthermore, the confusion matrix further measures model performance by
revealing possible patterns of misclassification. To ensure scalability
and feasibility for real-world applications, the models are also
evaluated in terms of running time and memory usage.

# Result

## Convolutional Neural Networks (CNNs)

### Basic CNN model

Applying image augmentation techniques yields noticeable improvements in
the performance of the basic CNN model. Specifically, using the
transformed dataset increases 2.6% of accuracy, 3% of precision, and 1
of F1 score than the original one **(Figure 3)**. Normalisation also accounted for most
of the results, noticeably doubling the precision. Given the fact that
normalisation reduces the differences between the variation range of the
different variables, error gradually decreases [@sola1997importance]
which improves both convergence and generalisation of the network
[@shao2020normalization].

Image augmentation techniques (random rotation and random flip), except
for increasing the accuracy 0.9%, has no impact on the precision and
even decreases the F1 score by 1 **(Figure 3)**. As these cells in medical images can
appear in many variations, these geometric transformations can introduce
variability into the model by changing the perspectives of objects
within images mitigating the overfitting issue encountered initially
[@xu2023comprehensive]. However, as both the angle of rotation and axis
of flipping directly affect the preservation label identity and
variations in the dataset [@xu2023comprehensive], the unfavourable
outcome might indicate that the chosen magnitude of these operations
within this project is not the most optimal one.



All the masking techniques, meanwhile, appear to contribute little to
the improvement of model performance. They even worsen the model's
accuracy by 0.5 - 0.7% **(Figure 3)**. Although the edge detection and blur type
identification techniques have been proven to have remarkable
performance on deep learning models [@wang2017blur, @guo2021non], the
simple architecture of the basic CNN causes its limited capacity and
sensitivity to input variations of architecture, which might lead to
less capability of benefiting from such complex image preprocessing
techniques like masking. The lack of the improvement could indicate that
the basic CNN model's architecture is not sophisticated enough to
capitalise on the increased variability without additional modification
[@tuggener2022enough].



```{r,echo = FALSE,fig.cap = "Basic CNN model under different data transformation and masking techniques",fig.width=3.5,fig.height=4, message=FALSE, warning=FALSE, fig.pos='h'}

library(jsonlite)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(gridExtra)
library(grid)

# Define directories and load metrics
lab_models <- c(
  "shiny/LabModel_nt_nm",
  "shiny/LabModel_n_nm",
  "shiny/LabModel_nrfrr_nm",
  "shiny/LabModel_nrfrr_cb",
  "shiny/LabModel_nrfrr_se",
  "shiny/LabModel_nrfrr_gau"
)
lab_list <- lapply(lab_models, function(dir) {
  load_and_prepare_metrics(dir, num_folds = 4, num_repeats = 1)
})

# Prepare data for plotting
original_vs_aug_data <- data.frame(
  Model = factor(c("Original", "Normalization", "Augmentation"), levels = c("Original", "Normalization", "Augmentation")),
  Accuracy = c(lab_list[[1]]$avg_accuracy, lab_list[[2]]$avg_accuracy, lab_list[[3]]$avg_accuracy),
  F1_Score = c(mean(lab_list[[1]]$avg_f1_scores), mean(lab_list[[2]]$avg_f1_scores), mean(lab_list[[3]]$avg_f1_scores)),
  Precision = c(mean(lab_list[[1]]$avg_precision), mean(lab_list[[2]]$avg_precision), mean(lab_list[[3]]$avg_precision))
)

masking_tech_data <- data.frame(
  Model = factor(c("No Mask", "Basic Boundary", "Sobel", "Gaussian Blur"), levels = c("No Mask", "Basic Boundary", "Sobel", "Gaussian Blur")),
  Accuracy = c(lab_list[[3]]$avg_accuracy, lab_list[[4]]$avg_accuracy, lab_list[[5]]$avg_accuracy, lab_list[[6]]$avg_accuracy),
  F1_Score = c(mean(lab_list[[3]]$avg_f1_scores), mean(lab_list[[4]]$avg_f1_scores), mean(lab_list[[5]]$avg_f1_scores), mean(lab_list[[6]]$avg_f1_scores)),
  Precision = c(mean(lab_list[[3]]$avg_precision), mean(lab_list[[4]]$avg_precision), mean(lab_list[[5]]$avg_precision), mean(lab_list[[6]]$avg_precision))
)

# Convert data to long format for ggplot2
original_vs_aug_data_long <- melt(original_vs_aug_data, id.vars = "Model")
masking_tech_data_long <- melt(masking_tech_data, id.vars = "Model")

# Define new custom colors
custom_colors <- c('#835AF1', '#7FA6EE', '#B8F7D4', "#c44e52")

# Function to extract legend
get_legend <- function(myplot) {
  tmp <- ggplotGrob(myplot)
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}

# Plot for Original vs Normalization vs Augmentation
p1 <- ggplot(original_vs_aug_data_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(0, 0.20)) +  # Set y-axis limit
  labs(
    fill = "Metric"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    axis.title.x = element_blank(),  # Remove x-axis title
    axis.title.y = element_blank(),  # Remove y-axis title
    axis.text.x = element_text(size = 6),
    axis.text.y = element_text(size = 6),  # Reduce y-axis text size
    legend.position = "none",
    strip.background = element_rect(fill = "lightblue", color = "black"),
    strip.text = element_text(size = 8),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )

# Plot for different masking techniques
p2 <- ggplot(masking_tech_data_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(0, 0.20)) +  # Set y-axis limit
  labs(
    fill = "Metric"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    axis.title.x = element_blank(),  # Remove x-axis title
    axis.title.y = element_blank(),  # Remove y-axis title
    axis.text.x = element_text(size = 6),
    axis.text.y = element_text(size = 6),  # Reduce y-axis text size
    legend.position = "none",
    strip.background = element_rect(fill = "lightblue", color = "black"),
    strip.text = element_text(size = 8),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )

# Create a dummy plot to extract the legend
dummy_plot <- ggplot(original_vs_aug_data_long, aes(x = Model, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = custom_colors) +
  labs(
    fill = "Metric"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 6),
    legend.key.size = unit(0.4, "cm"),
    legend.margin = margin(t = 5)
  )

# Extract legend from the dummy plot
legend <- get_legend(dummy_plot)

# Combine the plots into a 2-row layout and add the legend
grid.arrange(p1, p2, legend, nrow = 3, heights = c(2, 2, 0.3), top = textGrob("LabModel Metrics Comparison", gp = gpar(fontsize = 8, fontface = "bold")))

```

### Advanced CNN models (ResNet models)

Applying masking techniques on more advanced CNN models shows contrary
results, significantly improving the models' accuracy. Specifically,
including the cell boundary increases the accuracy by 3.3% on ResNet18
and 2.2% on ResNet50. Using sobel edge adds 4.3% more on the accuracy of
ResNet18, and 0.4% for ResNet50. Gaussian blur, which is also the
masking technique with best performance on advanced CNNs, better the
ResNet18's accuracy by 6.4% and ResNet50 by 5.16% **(Figure 7)**. Given
the deeply sophisticated architectures, these two networks appear to
gain significant advantages from these advanced image preprocessing
methods: masking to better their performance.

### Overall CNN models

Comparing the performance of three CNN models over 30 epochs based on
the optimal combination (**Figure 8**), the basic CNN model achieved a
validation accuracy of 15%, with both training and validation curves
remaining smooth. ResNet18 reached a validation accuracy of 17%,
stabilizing after initial fluctuations, while ResNet50 achieved
approximately 14.5%. None of the models exhibited overfitting,
indicating the effectiveness of our preprocessing methods.

Further comparing the F1 scores at the per-cluster level, the
ResNet18 model demonstrates superior performance with an average F1
score of 0.09, better than the average F1 score for ResNet50 (0.07) and
basic CNN model (0.05) **(Figure 9)**. This indicates that, on average, the ResNet18
balances precision and recall better than others. Specifically **(Figure 4)**, for
Cluster 6, ResNet18 achieves an F1 score of 0.45, which matches the
performance of the basic CNN. But ResNet18 also outperforms the basic
CNN in several other clusters (clusters 2 and 3), by achieving higher F1
scores, indicating its robustness and better generalization capabilities
compared to the basic CNN.



Overall, the ResNes18 would be the best model, and as ResNet50 and basic
CNN have similar accuracies but better F1-score, we suggest ResNet50
still outperforms basic CNN.

The good performance of ResNet18 can be attributed to its more complex
architecture, which includes additional convolutional and pooling
operations [@article], allowing it to capture more intricate features
compared to the basic CNN model.

Interestingly, ResNet50 did not outperform the basic CNN with its
accuracy but the F1-score, in the F1 distribution, while the basic CNN
model shows a higher F1 score for certain clusters, both ResNet provide
more comprehensive predictions across all clusters. We believed it could
be attributed to the observed imbalance in the data, where the basic CNN
model's tendency to over-predict certain clusters likely contributed to
its seemingly better performance in those specific cases. In addition to
this, as the ResNet50 is more complex with the architecture, the lack of
performance in ResNet50 compared to ResNet18 might still be due to the
model's tendency to over-predict certain clusters for ResNet18. The
imbalance issue also leads to its comparable or even lower performance.



```{r,echo=FALSE,fig.width=3.2,fig.height=4,fig.cap="F1 Scores by Cluster for all Models", fig.pos='h'}
# Visualization of F1 scores using ggplot2 with facets and horizontal bars
p <- ggplot(f1_data, aes(x = as.factor(Cluster), y = F1_Score, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(limits = c(0, 0.7)) +  # Set y-axis limit from 0 to 0.7
  labs(
    x = "Cluster",
    y = "F1 Score",
    fill = "Model"
  ) +
  facet_wrap(~ Model, ncol = 1, scales = "free_y") +  # Facet by Model with one column
  theme_minimal(base_size = 5.6) +  # Reduced base size by 20%
  theme(
    axis.title.x = element_text(margin = margin(t = 10), size = 5.6),
    axis.title.y = element_text(margin = margin(r = 10), size = 5.6),
    legend.position = "bottom",
    legend.title = element_text(face = "bold", size = 5.6),
    legend.text = element_text(size = 5),
    strip.background = element_rect(fill = "lightblue", color = "black", linewidth = 0.5),
    strip.text = element_text(face = "bold", size = 5.6),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_line(color = "grey90"),
    panel.spacing = unit(1, "lines")  # Increase the space between panels
  )

print(p)


```

## Transformers (ViT)

Under the single validation training **(Figure 5)**, the accuracy of the ViT model
increased steadily and continued to grow, while the validation accuracy
began to stabilise at around 17.5% after 10 epochs (Figure). In the
latter half of the training, we observed signs of overfitting, but the
validation curve did not decline significantly, it was considered as a
"good" overfitting.



Comparing the single validation training of the best CNN model
(ResNet18) **(Figure 5)**, ViT outperformed the CNN by approximately 4% in accuracy.
Additionally, ViT made predictions on more clusters (wider prediction)
and exhibited higher precision and recall both for each cluster and
overall **(Figure 10)**. Therefore, it could be concluded that the ViT model
simultaneously satisfies a wide range of predictions and recalls, and
demonstrates better performance. This improved performance can be
attributed to the generalisation capabilities, the ability to capture
global context, combined with its extensive pretraining on large
datasets.


```{r,echo = FALSE,fig.cap="Train and validation accuracy between ViT and ResNet18",warning=FALSE}
make_plot_1 <- function(folder_path) {

  json_filepath <- file.path(folder_path, paste0("/final_evaluation_metrics.json"))
  
  json_data = load_metrics(json_filepath)
  train_accuracies <- json_data$epoch_accuracies$train
  val_accuracies <- json_data$epoch_accuracies$val
}
```

```{r fig.width=3.5, fig.cap="Train and validation accuracy between ViT and ResNet18",fig.height=2,echo = FALSE, fig.pos='h'}
resnet18_path = "shiny/ResNet18_n_nodecay/final_model/final_evaluation_metrics.json"
vit_path = "shiny/VIT_n_weightdecay/final_model/final_evaluation_metrics.json"
  
json_data_1 = load_metrics(resnet18_path)
train_accuracies_1 <- json_data_1$epoch_accuracies$train[1:15]
val_accuracies_1 <- json_data_1$epoch_accuracies$val[1:15]
epoch <- c(1:15)

df_model_1 = data.frame(epoch, train_accuracies_1, val_accuracies_1)

p1 = df_model_1 |>
  ggplot(aes(x = epoch)) +
  geom_line(aes(x = epoch, y = train_accuracies_1, color = "Train")) +
  geom_line(aes(x = epoch, y = val_accuracies_1, color = "Validation")) +
  labs(color = "", x = "Epoch", y = "Accuracy", title = "ResNet18") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9),  # Adjust title size here
    axis.title.x = element_text(size = 6),
    axis.title.y = element_text(size = 6),
    axis.text.x = element_text(size = 6),
    axis.text.y = element_text(size = 6),
    legend.position = "bottom",
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 8)
  )

json_data_2 = load_metrics(vit_path)
train_accuracies_2 <- json_data_2$epoch_accuracies$train
val_accuracies_2 <- json_data_2$epoch_accuracies$val
df_model_2 = data.frame(epoch, train_accuracies_2, val_accuracies_2)

p2 = df_model_2 |>
  ggplot(aes(x = epoch)) +
  geom_line(aes(x = epoch, y = train_accuracies_2, color = "Train")) +
  geom_line(aes(x = epoch, y = val_accuracies_2, color = "Validation")) +
  labs(color = "", x = "Epoch", y = "Accuracy", title = "ViT") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 9),  # Adjust title size here
    axis.title.x = element_text(size = 6),
    axis.title.y = element_text(size = 6),
    axis.text.x = element_text(size = 6),
    axis.text.y = element_text(size = 6),
    legend.position = "bottom",
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 8)
  )

ggarrange(p1, p2, ncol=2, common.legend=TRUE, legend="bottom")
```


## Shiny

Key findings are then visualized and presented using an interactive
[platform Shiny](https://gitparth12.shinyapps.io/data3888_imaging_021/), which enables a more engaging and informative approach
to the end-users. The app has three main functions **(Figure 6)**, including:

1.  Providing insights of CNNs performance (accuracy, precision, F1
    score, etc.). Considering that the dataset has a huge class
    imbalance problem, Shiny also displays models' performance both
    overall and across 28 clusters.

2.  Providing insights of the Transformers (ViT), with detailed
    comparison with the best model from CNN class: ResNet18.

3.  Predicting clusterID and corresponding predicted probability of cell
    image uploaded by users, using selected combination of model and
    data transformation, masking techniques.

## Discussion and Conclusion

The project findings highlight the importance of selecting appropriate
data transformations and model architectures for the task of cell image
classification. Normalisation was an universally beneficial
preprocessing step, while the effectiveness of image augmentation and
masking techniques varied depending on model complexity. The basic CNN
model's limitations were highlighted by its inability to benefit from
more complex preprocessing methods, whereas advanced CNN models
demonstrated significant improvements when these techniques were
applied. Overall, ViT appears as the best model with highest prediction
performance.

However, it is critical to acknowledge that the success of deep learning
in vision depends on three factors: high capacity models, computational
power, and availability of large-scale labelled data
[@sun2017revisiting]. The project's scope and methodology still has
several limitations. Specifically, the small dataset size constrained
the complexity of the models that could be trained, as more
sophisticated models require larger datasets to be effective. The
significant imbalance issue across clusters might also potentially
introduce certain biases into the models. Additionally, the vague nature
of the cluster labels posed a challenge for model interpretability,
making it difficult to understand the basis for the model's predictions.
Other shortcomings, such as the low model complexity due to limited
computing resources and the lack of interpretability, highlight areas
for future work to improve the methodology approach.

Therefore, future work can leverage more data from under-represented
classes to address the class imbalance issues such as a dataset
including the full brain to provide a richer and more varied set of
images for analysis. More advanced and deeper network architectures
should also be considered for better prediction given the complexity of
medical visuals such as cell images. Given that ViT outperforms all CNNs
being considered, more networks from this class can be trained to
investigate the efficiency of Transformers in medical image analysis.
Additionally, the application of cross-validation (CV) across all models
would ensure a more robust evaluation of their performance.


\section*{Group Contribution}

**Parth Bhargava (510401900):** Converted the Imaging lab set up code from R to Python. Trained some basic CNNs (with J). Worked on Shiny’s backend: matched the project’s result with the UI’s design of models’ performance, Image prediction tab. Generated some plots for the final report. Structurised and arranged the group's GitHub repository.

**MiaoMiao Chen (520181814):** Initial exploration with three masking techniques, the initial ResNet50 model, the basic machine learning models with Hazel, workflow figure, the Methodology figures (with Hazel), part of Shiny design, part of script, part of presentation slides, responsible for the meeting minutes submission, report's methodology, second half of report's results,report formatting in r.

**Ethan Chang (520406670):** I took notes during workshops and consultations, refined and created slides based on tutor feedback, helped with initial lab model transformations, and improved the presentation script for clarity and flow, ensuring it stayed within the time limit. I created the report abstract, co-wrote the executive summary and improved phrasing.

**Hazel Nguyen (510652339):** Fitting KNN, Random Forest (with Miaomiao). Two main figures: Workflow & Methodology (with Miaomiao). Part of the Shiny's UI design. Presentation slides (including recorded video of Shiny) and part of the script. Report template and rendering using R's pinp, first half of the report's result, part of the discussion and conclusion.

**Evan Nguyen (500469361): **I was responsible for writing and editing the report and the presentation script. Additionally I also helped with initial data exploration and assisted with looking into some basic transformation techniques for cells.

**Jiacheng Yu (510050786):** I primarily managed the architecture and most of the fine-tuning and training process for deep learning models, concluded save and visualization. Moreover, I explored the shiny app for Python. and built the code architecture for the whole project. Also, I drew about half of the diagrams in the final report.

\newpage

# Github and Shiny Link

<https://github.com/jiyu4399/DATA3888_Image_Group_2>

<https://gitparth12.shinyapps.io/data3888_imaging_021/>

# Appendix



```{=tex}
\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.4\textwidth]{shiny.png}
    \caption{Shiny}
    \label{fig:Shiny}
\end{figure}
```



```{r,echo=FALSE,fig.height=2.1,fig.width=2.3,fig.cap="Models accuracy under different masking techniques", fig.pos='h'}
# Generate the accuracy data frame with updated techniques
accuracy_data <- prepare_accuracy_data(results_list, directories, target_models, new_names)

# Reorder the techniques and models
accuracy_data$Technique <- factor(accuracy_data$Technique, levels = c("No Mask", "Basic Boundary", "Sobel", "Gaussian Blur"))
accuracy_data$Model <- factor(accuracy_data$Model, levels = c("Simple CNN", "ResNet50", "ResNet18"))

# Create the horizontal bar plot for accuracy distribution by masking technique
p_acc <- ggplot(accuracy_data, aes(x = Technique, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +  # Use geom_bar for bar plot
  coord_flip() +  # Rotate the plot
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.20)) +  # Set y-axis as percentage with upper limit 20%
  labs(
    x = "Masking Technique",
    y = "Accuracy",
    fill = "Model"
  ) +
  theme_minimal(base_size = 8) +  # Adjust base size for better readability
  theme(
    axis.title.x = element_text(margin = margin(t = 6), size = 8),
    axis.title.y = element_text(margin = margin(r = 6), size = 8),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7),
    legend.position = "bottom",
    legend.title = element_text(face = "bold", size = 8),
    legend.text = element_text(size = 7),
    legend.box = "vertical",
    legend.box.just = "left",
    legend.direction = "horizontal",
    legend.key.size = unit(0.5, "lines"),
    strip.background = element_rect(fill = "lightblue", color = "black"),
    strip.text = element_text(face = "bold", size = 8),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank(),
    plot.margin = margin(10, 10, 10, 10)  # Adjust plot margins
  ) +
  guides(fill = guide_legend(nrow = 3, byrow = TRUE))  # Arrange legend in three rows

print(p_acc)

```



```{r fig.width=3, fig.height=7, fig.cap="Train and validation accuracy for all models",fig.height=3.7,echo = FALSE, fig.pos='h'}

basic_cnn_path = "shiny/LabModel_nrfrr_nm/Repeat_1_Fold_4/evaluation_metrics.json"
resnet18_path = "shiny/ResNet18_nrfrr_gau/Repeat_1_Fold_3/evaluation_metrics.json"
resnet50_path = "shiny/ResNet50_nrfrr_gau/Repeat_1_Fold_1/evaluation_metrics.json"

epoch <- c(1:30)

json_data_1 = load_metrics(basic_cnn_path)
train_accuracies_1 <- json_data_1$epoch_accuracies[1,]
val_accuracies_1 <- json_data_1$epoch_accuracies[2,]

json_data_2 = load_metrics(resnet18_path)
train_accuracies_2 <- json_data_2$epoch_accuracies[1,]
val_accuracies_2 <- json_data_2$epoch_accuracies[2,]

json_data_3 = load_metrics(resnet50_path)
train_accuracies_3 <- json_data_3$epoch_accuracies[1,]
val_accuracies_3 <- json_data_3$epoch_accuracies[2,]

df = data.frame(epoch, train_accuracies_1, val_accuracies_1, train_accuracies_2, val_accuracies_2, train_accuracies_3, val_accuracies_3)

title_size = 8
axis_size = 6

p1 <- df |>
  ggplot(aes(x = epoch)) +
  geom_line(aes(x = epoch, y = train_accuracies_1, color = "Train")) +
  geom_line(aes(x = epoch, y = val_accuracies_1, color = "Validation")) +
  labs(
    title = "Basic CNN",
    x = "Epoch",
    y = "Accuracy"
  ) +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),
        axis.title.x = element_text(size = title_size),            # Adjust x-axis label font size
        axis.title.y = element_text(size = title_size),
        legend.title = element_text(size = title_size),
        legend.text = element_text(size = title_size))

p2 <- df |>
  ggplot(aes(x = epoch)) +
  geom_line(aes(x = epoch, y = train_accuracies_2, color = "Train")) +
  geom_line(aes(x = epoch, y = val_accuracies_2, color = "Validation")) +
  labs(
    title = "ResNet18",
    x = "Epoch",
    y = "Accuracy"
  ) +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),
        axis.title.x = element_text(size = title_size),            # Adjust x-axis label font size
        axis.title.y = element_text(size = title_size),
        legend.title = element_text(size = title_size),
        legend.text = element_text(size = title_size))

p3 <- df |>
  ggplot(aes(x = epoch)) +
  geom_line(aes(x = epoch, y = train_accuracies_3, color = "Train")) +
  geom_line(aes(x = epoch, y = val_accuracies_3, color = "Validation")) +
  labs(
    title = "ResNet50",
    x = "Epoch",
    y = "Accuracy"
  ) +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),  # Adjust title font size and center it
    axis.title.x = element_text(size = title_size),            # Adjust x-axis label font size
    axis.title.y = element_text(size = title_size),
    legend.title = element_text(size = title_size),
    legend.text = element_text(size = title_size))           # Adjust y-axis label font size) +


ggarrange(p1, p2, p3, ncol=1, nrow=3, common.legend=TRUE, legend="bottom")
```


```{r,echo=FALSE,fig.height=2,fig.width=2.5,fig.cap="Average F1 Score Distribution for all models",warning=FALSE, fig.pos='h'}

p_box <- ggplot(f1_data, aes(x = Model, y = F1_Score, fill = Model)) +
  geom_boxplot(width = 0.4, outlier.size = 0.3) +  # Increase the width of the boxplot and set outlier size
  geom_jitter(shape = 16, position = position_jitter(width = 0.2), size = 0.2, alpha = 0.4) +  # Further adjust jitter points
  scale_fill_manual(values = generate_custom_colors()) +
  scale_y_continuous(limits = c(0, 0.7)) +  # Set y-axis limit from 0 to 0.7
  labs(
    x = "Model",
    y = "F1 Score",
    fill = "Model"
  ) +
  theme_minimal(base_size = 7) +  # Adjust base size for better readability
  theme(
    axis.title.x = element_text(margin = margin(t = 10), size = 8),  # Adjust x-axis title size
    axis.title.y = element_text(margin = margin(r = 10), size = 8),  # Adjust y-axis title size
    axis.text.x = element_text(size = 8),  # Adjust x-axis text size
    axis.text.y = element_text(size = 8),  # Adjust y-axis text size
    legend.position = "none",
    strip.background = element_rect(fill = "lightblue", color = "black", linewidth = 0.5),
    strip.text = element_text(face = "bold", size = 7),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_line(color = "grey90")
  )

print(p_box)

```


\vspace{10 cm} 

.

```{r fig.width=3.2, fig.cap="F1 score per cluster for Vit",fig.height=5,echo = FALSE, fig.pos='h'}
vit_path = "shiny/VIT_n_weightdecay/final_model/final_evaluation_metrics.json"

json_data = load_metrics(vit_path)
precision = json_data$`Precision by cluster`
recall = json_data$`Recall by cluster`
f1_score = json_data$`F1 score by cluster`
clusters = as.factor(c(1:28))
df = data.frame(clusters, precision, recall, f1_score)
p1 <- ggplot(df, aes(x = clusters, y = precision)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "ViT Metrics by Cluster",
       x = "Cluster",
       y = "Precision") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_text(size = 6),  # Change the size as needed
    axis.title.y = element_text(size = 8)   # Change the size as needed
  )

p2 <- ggplot(df, aes(x = clusters, y = recall)) +
  geom_bar(stat = "identity", fill = "#ff7f0e") +
  labs(x = "Cluster",
       y = "Recall") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 6),  # Change the size as needed
    axis.title.y = element_text(size = 8)   # Change the size as needed
  )

p3 <- ggplot(df, aes(x = clusters, y = f1_score)) +
  geom_bar(stat = "identity", fill = "#c44e52") +
  labs(x = "Cluster",
       y = "F1 Score") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 6),  # Change the size as needed
    axis.title.y = element_text(size = 8)   # Change the size as needed
  )

ggarrange(p1, p2, p3, nrow = 3, ncol = 1, common.legend = TRUE, legend = "bottom")

```
















\vspace{10 cm} 

.




```{r fig.width=2.8, fig.height=15, fig.cap="confusion matrix",fig.height=3.7,echo = FALSE, fig.pos='h'}

title_size = 8
axis_size = 8
axis_text_size = 4.5

basic_cnn_path = "shiny/LabModel_nrfrr_nm/Repeat_1_Fold_4/evaluation_metrics.json"
resnet18_path = "shiny/ResNet18_nrfrr_gau/Repeat_1_Fold_3/evaluation_metrics.json"
resnet50_path = "shiny/ResNet50_nrfrr_gau/Repeat_1_Fold_1/evaluation_metrics.json"

json_data_1 = load_metrics(basic_cnn_path)
conf_1 = json_data_1$confusion_matrix

json_data_2 = load_metrics(resnet18_path)
conf_2 = json_data_2$confusion_matrix

json_data_3 = load_metrics(resnet50_path)
conf_3 = json_data_3$confusion_matrix

confusion_1_long <- melt(conf_1)
colnames(confusion_1_long) <- c("TrueLabel", "PredictedLabel", "Count")

# Convert labels to factors for better control in plotting
confusion_1_long$TrueLabel <- factor(confusion_1_long$TrueLabel, levels = rev(1:28))
confusion_1_long$PredictedLabel <- factor(confusion_1_long$PredictedLabel, levels = 1:28)

# Create the heatmap
heatmap_1 <- ggplot(confusion_1_long, aes(x = PredictedLabel, y = TrueLabel, fill = Count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Basic CNN",
       x = "Predicted Label",
       y = "True Label",
       fill = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),
    axis.title.x = element_text(size = axis_size),
    axis.title.y = element_text(size = axis_size),
    axis.text.x = element_text(size = axis_text_size),
    axis.text.y = element_text(size = axis_text_size),
    legend.title = element_text(size = title_size),
    legend.text = element_text(size = axis_size))

confusion_2_long <- melt(conf_2)
colnames(confusion_2_long) <- c("TrueLabel", "PredictedLabel", "Count")

# Convert labels to factors for better control in plotting
confusion_2_long$TrueLabel <- factor(confusion_2_long$TrueLabel, levels = rev(1:28))
confusion_2_long$PredictedLabel <- factor(confusion_2_long$PredictedLabel, levels = 1:28)

# Create the heatmap
heatmap_2 <- ggplot(confusion_2_long, aes(x = PredictedLabel, y = TrueLabel, fill = Count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "ResNet18",
       x = "Predicted Label",
       y = "True Label",
       fill = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),
    axis.title.x = element_text(size = axis_size),
    axis.title.y = element_text(size = axis_size),
    axis.text.x = element_text(size = axis_text_size),
    axis.text.y = element_text(size = axis_text_size),
    legend.title = element_text(size = title_size),
    legend.text = element_text(size = axis_size))


confusion_3_long <- melt(conf_3)
colnames(confusion_3_long) <- c("TrueLabel", "PredictedLabel", "Count")

# Convert labels to factors for better control in plotting
confusion_3_long$TrueLabel <- factor(confusion_3_long$TrueLabel, levels = rev(1:28))
confusion_3_long$PredictedLabel <- factor(confusion_3_long$PredictedLabel, levels = 1:28)

# Create the heatmap
heatmap_3 <- ggplot(confusion_3_long, aes(x = PredictedLabel, y = TrueLabel, fill = Count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "ResNet50",
       x = "Predicted Label",
       y = "True Label",
       fill = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = title_size, hjust = 0.5),
    axis.title.x = element_text(size = axis_size),
    axis.title.y = element_text(size = axis_size),
    axis.text.x = element_text(size = axis_text_size),
    axis.text.y = element_text(size = axis_text_size),
    legend.title = element_text(size = title_size),
    legend.text = element_text(size = axis_size))

ggarrange(heatmap_1, heatmap_2, heatmap_3, nrow=3, ncol=1, legend = "bottom", common.legend = TRUE)
```






\newpage


